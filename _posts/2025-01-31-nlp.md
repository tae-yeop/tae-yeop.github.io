


## 옛날 방식

### Word2Vec

- 사전학습 언어 모델

단어의 의미를 미리 알아두자 ⇒ 다양한 task에 잘 사용할 수 있을 것이다

기계에게 단어의 의미를 어떻게 입력시킬 것인가

embedding space의 vector 형태로 나타낸다.

단어의 의미는 주변 단어에 의해 된다라는 언어학적 가정
![Image](https://github.com/user-attachments/assets/b2a6ed0a-05c4-486a-ab09-0b042de8cbad)
똑같이 뒤에 student, cook이 있는데 I 다음에는 am, You 다음에는 are ⇒ am, are과 비슷할 것이다

![Image](https://github.com/user-attachments/assets/5bc81a1e-068e-4192-8604-29e68162eaa4)
Skip-gram이 더 뛰어나다

- 하나를 보고 주변의 여러개를 예측하는게 더 어렵다
- 여러개를 보고 하나를 예측하는 것은 상대적으로 쉽다
- 목적이 임베딩을 얻는 것이므로 하나를 보고 여러개를 예측할 수 있는 방법이 더 적합하다



Tokenizer

단어를 보고 특정 인덱스를 뽑아준다.

이 인덱스를 이용해서 나중에 임베딩을 만들 것이기 때문

수정할때는 vocab.txt에 내가 단어를 넣도록 한다.

```python
[PAD]
[UNK]
[CLS]
[SEP]
[MASK]
A
B
C
D
E
F
G
H
I
K
L
M
N
O
P
Q
R
S
T
U
V
W
X
Y
Z
-
.
```
- 대표적인 토큰
{"unk_token": "[UNK]", "sep_token": "[SEP]", "pad_token": "[PAD]", "cls_token": "[CLS]", "mask_token": "[MASK]"}




## LLM

### BERT
BERT는 3가지를 한다

- 1)원래 것을 예측
- 2) Mask
- BERT는 15프로를 마스크시킨다
- 3) Graft
- graft를 시킬 수 도 있다 : 다른게 들어왔는데 원래는 무엇일까? , 노이즈가 있을 떄 노이즈를 제거하는 것과 비슷