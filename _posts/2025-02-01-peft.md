
- 말그대도 효율적으로 파인튜닝 하는 방법들들
- 모델이 너무 커서 전체 weight를 학습하기엔 메모리가 부족
- 혹은 파인튜닝 자체의 문제점 (catastrophic forgetting)이 있음
- 이를 위해 대부분 파라미터를 프리징하고 일부만 파인튜닝

- 예를 들어 llama 7B 모델
    - 로딩하면 7*1e9 ⇒ float32기준 28GB (1B params 당 4GB 메모리)
    - fp16 기준 학습 비용(intermediate activation 고려하지 않고도)
    - 1. weight : 2 bytes
    - 2. gradients : 2 bytes
    - 3. Stateful Optimizer : 4 bytes + 8bytes
    - 결국 파라미터당 16 bytes 필요 -> 112GB 필요

- https://devocean.sk.com/blog/techBoardDetail.do?ID=164779&boardType=techBlog

### LoRA

- AdaLoRA
- LLaMa-Adapter
- IA3




- Qunatization을 쓰는 방법도 있는데 이는 에러


### 라이브러리
- 허깅페이스에서 peft 라이브러리를 제공함
- `transformers`, `diffusers`랑도 호환이 잘됨
- 여러 예시는 [여기](ttps://github.com/huggingface/peft/tree/main/examples)

```python


```