---
title:  "딥러닝 모델 아키텍처"
excerpt:  ""
categories: 
- Deep Learning

tags:
- Architecture
 
toc_label: TOC
toc: true
toc_sticky: true
 
date: 2025-01-05

---

# 아키텍처 변천사
딥러닝 모델이 성공 여부는 익히 알다시피 데이터, 모델, 학습 방법 이 3가지에 달려 있다.
데이터는 데이터 자체 퀄리티와 다양성 그리고 라벨링의 여부가 중요하고 결국 돈이나 사람의 힘을 쓰던지해서
해결하는 문제이다. 학습 방법은 태스크나 프레임워크에 따라 달라질 수 있고 이 부분은 보통
이론이 어느정도 깔리면 구체적으로 데이터를 어떻게 공급할지, learning rate를 어떻게 할지 등 엔지니어링이
담당하게 된다.
마지막으로 모델, 즉 아키텍처도 중요한데 이 영역은 발전이 정체된거 처럼 보이기도 한다.
결국엔 계속 돌리다보면 UAT에 근거해서 수렴하게 될 것이라는 믿음 때문인지 이 부분은 핵심적인 발전은 느낌상
트랜스포머가 도입된 이후로 큰 맥락 상에선 변화가 없어 보이기도 한다.

# 빌딩 블록
- 연산자 레이어
- 비선형 레이어
사실상 모든 대부분 딥러닝 레이어는 MLP처럼 생각할 수도 있다.
따라서 딥러닝 모델의 가장 일반화된 다음 수식은 모든 아키텍처를 포괄한다고 볼 수 있다


그렇다면 디자인시 고려할 사항은 뭘까?
1) Adaptiveness (Adaptive Computation)가 중요하지 않을까?
Adaptiveness를 반영한 아키텍처를 다음과 같이 고려할 수 있다

HyperNet (PR-043)

- 데이터로 인해 네트워크 동작(Parameter)를 바꿔서 동작시킨다
- An approach of using one network to generate the weight for another network

PonderNet (https://arxiv.org/abs/2107.05407)

- feature를 보고 네트워크 동작을 얼만큰 더할지를 결정한다 (시간 축으로 Adaptive)
- recurrency
- can decide when to stop comptutation
- works as if-statement

Attention

- 데이터 자체를 보고 feature를 계속 Adaptive하게 바꾼다.

2) Invariance 
- Transformer architecture는 attention 매커니즘을 기반으로 하는 permutation invariant architecture이다. 
- 그러다 보니 이를 어느정도 상쇄해야 할 경우 PE를 넣기도 한다. 이런것을 고려할 것


3) 최종 아웃풋
무엇을 표현 하고자 하는가에 맞춰 나누는게 좋지 않을까? 예를 들어 GNN은 graph라는 concept을 표현하려고 하는 conceptual model이다.

4) 최적화 가능성
- 복잡도만 높이는건 좋은게 아니다. 
Singular Value Perturbation and Deep Network Optimization, Rudolf H. Riedi et al., 연구에선 관련성이 있음을 보여준다

![Image](https://github.com/user-attachments/assets/805b4856-13ff-4e2c-b290-b6f5a3e89042)


즉 loss surface를 체크하면서 해당 구조가 학습하기 쉬운지를 판단해보는것도 좋다
Visualizing the Loss Landscape of Neural Nets 연구 참조
실제 시각화 코드는 다음을 참조
- https://github.com/tomgoldstein/loss-landscape
- https://github.com/marcellodebernardi/loss-landscapes
- https://github.com/artur-deluca/landscapeviz

이론적으로 Random matrix theory와 연관해서 많이 연구되는 듯하다.
결국 이런 것들은 Hessian matrix를 살펴보는 것으로 분석을 하게 된다.

Geometry of Neural Network Loss Surfaces via Random Matrix Theory
Random matrix theory and the loss surfaces of neural networks
An Investigation into Neural Net Optimization via Hessian Eigenvalue Density, Behrooz Ghorbani et al.

6) Weight


7) Spectral bias




# 전체 개괄 비교
ConvNet

- high pass filter
- texture > shape

MSA

- low pass fileter
- shape > texture

단순히 네트워크의 breath와 depth를 무작정 늘린다고 성능이 보장되지 않는다. 

- RegNet paper
- How Vision Transformer works?


Conv라고 무조건 빠르진 않다
grouped conv는 transformer보다 느리다.  https://www.reddit.com/r/MachineLearning/comments/lqpkgb/transformer_flops_vs_cnn_flops_speed_r/

groped conv는 일반 conv보다 5~6배 느리다.

efficientnetb0 ~5m params, 30G Flops,  (32,3,256,456)  가 transformer ~5m params, 40G Flops, (1,1000,256) - (BATCH, TIME, DIM) 보다 training step 3.5배 느리다.


데이터 타입에 따라 굳이 특정 패밀리의 레이어를 쓰는 것은 많이 무뎌진듯
모두 Conv로 처리할수도 있고 모두 MLP(attention)으로도 할 수 있다.



# MLP 레이어


# Convolution 레이어

뒤에 나오지만 CNN은 결국 Toeplitz 행렬을 implicitly하게 쓴 것이다 (이것이 inductive bias)




## Toeplitz
CNN의 kernel의 일반화된 형태이다.
Sequential modeling에 효과적

- https://github.com/OpenNLPLab/Tnn
- SKI to go Faster: Accelerating Toeplitz Neural Networks via Asymmetric Kernels



# RNN 계열

## LSTM

파이토치에서 `nn.LSTMCell`을 쓰는 경우

셀별 루프가 돌기 때문에 CUDA 커널 재호출이 일어나서 느리다.

`nn.LSTM(stacked)` 한 방으로 묶어 커널 호출 1회로 줄이기

“순수 언어 모델·시계열 예측”처럼 길고 고정된 시퀀스를 한꺼번에 GPU에 태울 수 있는 작업엔 nn.LSTM(스택)-이 훨씬 빠르고 단순

---


![Image](https://github.com/user-attachments/assets/41f7d656-9d23-4f00-be33-8f26245f6cfc)


# 조합 블록
## Residual Block

## Inception Block
- split-transform-merge 원리
- computational cost는 낮추고 더 높은 representational power를 얻음


### ResNext
- Aggregated Residual Transformations for Deep Neural Networks



# More Context!
- 단순히 인풋 자체만 작은 receptive field로 연산하기 보다는 




## Retentive Network(RetNet)
- Retentive Network: A Successor to Transformer for Large Language Models
- [https://kimjy99.github.io/논문리뷰/retnet/](https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/retnet/)
- https://github.com/Jamie-Stirling/RetNet

![Image](https://github.com/user-attachments/assets/e13c3134-7731-43aa-9a67-1b95ff13415f)
- 이 친구도 SSM처럼 forward를 여러개로 구분했음
- 긴 시퀀스 모델링을 위해 이렇게 함
- 훈련할 때에는 병렬모드를 통해 

- 아키텍처 레이아웃은 Transformer 와 유사 : residual connection, pre-LayerNorm ⇒ L개의 동일한 RetNet 블록
- RetNet 블록에는 multi-scale retention (MSR) 모듈과 feed-forward network (FFN) 모듈이라는 두 개의 모듈

- Retention Mechanism
    - ChunkwiseRetention and GatedMultiScaleRetention를 사용한다
    - 이 둘은 인풋을 청크단위로 처리하여 새로운 인풋이 오면 정보를 유지하거나 버릴지 결정한다
    - 과거 정보에서 중요한 것을 유지하고 새로운 인풋을 효과적으로 합쳐서 사용한다
    - Attention-free이다


# 비선형 활성 레이어
- 요즘에는 표준화되어서 사용하는 것만 사용하는 느낌
- 크게 발전은 없고 더 복잡한걸로 바꾸어도 성능 변화는 미비한 경우도 많은 것 같다.
- 아래 정도만 알고 나중에 가면 또 SOTA 레이어가 나올 수 있을수도?
- 대체로 SiLU, GELU 정도만 써도 거의 최선의 결과
- 그림 출처 : https://courses.grainger.illinois.edu/ece417/fa2020/slides/lec08.pdf

## 1) Sigmoid
$$
\text{sigmoid}(x) = \frac{1}{1+\exp{(-x)}}
$$

Sigmoid는 argument가 너무 크거나 낮을 경우 saturation이 일어난다. 즉, 어느 정도 크거나 작으면 하면 하나의 값 0, 1로 몰려서 값의 변화에 무감각해진다. 

## 2) ReLU 계열

![My Image](https://github.com/user-attachments/assets/8668917d-3744-49be-9e56-0f098e630dc5 "Optional Title")



Vanishing gradient 문제를 해결하는데 쓰인다. 어떤 positive 값을 지니던지 backprop이 계속 동작한다. 

단점은 음수가 나오면 hidden node가 꺼지게 되고 학습이 중단된다. 이를 Dying ReLU라고 부른다. 이런 문제를 해결하기 위한 대체가능한 activation들이 있다.

그외 ## LeakyReLU

gradient가 일정하고 piece-wise linear형태로 결과가 출력된다. 하지만 negative part가 dataset에 매칭되지 않을 가능성이 있다.
PReLU(Parametric ReLU)




## 3) Softplus

ReLU의 smooth approximation으로 항상 양수이다. x가 $-\infty$에 가까울수록 gradient는 0에 가까워 진다. 이름의 유래는 soft version of $x^{+}=\max(0,x)$에서 나왔다. 

$$
f(x) = \ln{(1 + e^{x})}
$$



![My Image](https://github.com/user-attachments/assets/a0d998a8-efa5-451e-97b3-07d7f6ac50af)

BCE Loss에서 log probabilities를 계산하는데 사용될 수도 있다. 사실상 sigmoid 형태로 표현하는 것과 마찬가지이다. 0,1은 각각 class label을 뜻한다.

$$
\log{\text{prob}_1(x)}= \log{p(x)} = -\text{softplus}(-x) \\
\log{\text{prob}_0(x)}= \log{1-p(x)} = -\text{softplus}(x)
$$

![My Image](https://github.com/user-attachments/assets/d311a6cf-46a0-4064-a302-6ceae4f3176c "Optional Title")



## 4) ELU, GELU(Gaussian Error Linear Unit)
- BERT, ROBERTa, ALBERT 등 최신 모델에서 많이 사용됨
- layer가 깊어질수록 adaptive dropout 형태로 입력치에 가중치를 부여하여 zoneout되는 형태로 

![My Image](https://github.com/user-attachments/assets/477adde2-8f76-4042-a1b8-1d86c53e93d6 "Optional Title")




## 6) Swish(SiLU)
- SiLU, Swish 같은거나 마찬가지
- 베타 파라미터로 좀 더 조절할 수 있냐 아니냐 차이 정도
- Swish가 몇몇 케이스에서 성능이 더 좋다고 한다. 
- 자세한 건 여길 참조하자 : https://blog.paperspace.com/swish-activation-function/


```python
def SiLU(x):
    return x * sigmoid(x)

def swish(x, beta=10):
    return x * sigmoid(beta * x)
```

## 7) Mish
- 2020년 8월경
- [Mish: A Self Regularized Non-Monotonic
Activation Function](https://arxiv.org/pdf/1908.08681)
- 사용된 케이스 : 
- gradient가 좀 더 부드럽다고 함

![img1 daumcdn](https://github.com/user-attachments/assets/db4647d6-2da2-4480-b26a-7bb751a2bd1f)

- 레이어도 깊어져도 좋은 결과를 냄
![img1 daumcdn](https://github.com/user-attachments/assets/ac9347a4-7655-4b97-9cf9-fa3649e59924)

```python
def mish(x) :
    return x * tf.nn.tanh( tf.nn.softplus(x)) 

```


# Special Layers
## BlurPool
- 블러 계산을 통해 성능을 높이는 사례가 있다 (eg.StyleGAN)
- 특히 [Richard Zhang 논문](https://arxiv.org/abs/1904.11486)에서 이 효과에 대해 자세히 설명해줌
- BlurPooling을 하는건 Anti-aliasing 효과를 내는데 이는 결국 shift-equivariance는 높이는 효과
- 기존의 maxpooling, average pooling, and strided convolution는 shift-equivariance 무너뜨림






- 또 블러가 앙상블 같은 효과를 낸다는 보고가 있다 ([Blurs Behave Like Ensembles: Spatial Smoothings to Improve Accuracy, Uncertainty, and Robustness](https://arxiv.org/abs/2105.12639))


# 아키텍처와 Weight간의 관계




# 시간 순 정렬



# 최신
Transformer square

- https://arxiv.org/abs/2501.06252

