---
title:  "Self-Supervised Learning"
excerpt:  "자기지도 학습 정리"
categories: 
- Self-Supervised Learning

tags:
- Self-Supervised Learning
 
toc_label: TOC
toc: true
toc_sticky: true
 
date: 2025-01-11
---

- 일종의 Representation Learning의 하위 개념이라고 생각하면 됨
- 활용 용도는 라벨링하지 않는 대규모 데이터셋에 Pretraining 시킨 뒤에 Donwstream task에 finetuning하는식으로 할 수 있음
- 혹은 임베딩을 뽑아내는 Feature Extractor로도 사용할 수 있음
- 



## [I-JEPA](https://arxiv.org/abs/2301.08243)
- (23.1)
- SSL 분야를 선도하는 얀 르쿤 교수님이 트위터에서 많이 홍보한 연구
- 이미지에 대한 representation을 좋은 임베딩을 얻고자 함

![Untitled](https://github.com/user-attachments/assets/3d6937b6-0859-489f-9967-2bff31a35215)

- single context block을 사용해서 다양한 타겟 블록을 예측한다
- context encoder : ViT로써 visible context patches만 처리함
- The predictor : narrow ViT로써 positional tokens을 조건으로 받고 encoder 결과물을 받아서 해당 위치의 타겟블록 representation을 예측함
    - target representations은 target-encoder의 아웃풋에 대응된다
- target encoder : context encoder weights로 EMA로 업데이트된다

- 임베딩을 잘 학습했는데 linear probing을 해봄
- linear probing은 쉽게 말해 linear layer를 헤드에 달아서 classification task를 얼마나 잘 수 수행하는지 살펴보는 것



## [DoRA](https://arxiv.org/abs/2310.08584)
- 23.12 (ICRL 2024 Oral)



# 활용
- 그래서 실제로 어떻게 활용할 수 있는지가 중요
- 연구 논문용으로는 뭐 이리 볶고 저리 볶고 할 수 있겠지만 서비스 개발로 연결지을 만한 요소가 있을까?

## [Object Detection](https://arxiv.org/abs/2410.07442)
- 24년도 10월
- 실제 SSL을 한 백본을 Detection에 파인튜닝했을 때 스크래치 대비 성능 향상이 있음을 보여줌 (테이블4 참조)

- [Are Large-scale Datasets Necessary for Self-Supervised Pre-training?](https://arxiv.org/pdf/2112.10740)
- 여기서도 Random Init 대비 SSL을 적용한게 성능이 더 좋다고 리포트함
- 