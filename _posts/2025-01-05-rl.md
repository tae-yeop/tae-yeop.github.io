---
title:  "ê°•í™”í•™ìŠµ ê¸°ì´ˆì •ë¦¬í•´ë³´ê¸°"
excerpt:  "ê°•í™”í•™ìŠµ ë‚´ìš© ì •ë¦¬"
categories: 
- Reinforcement Learning

tags:
- Reinforcement Learning
 
toc_label: TOC
toc: true
toc_sticky: true
 
date: 2025-01-05

---

# Basics

## RL êµ¬ì„±ìš”ì†Œ
- ê°•í™”í•™ìŠµì—ì„œ ìƒì •í•˜ëŠ” ê¸°ë³¸ì ì¸ ì…‹íŒ…ì´ ìˆëŠ”ë° ì´ë¥¼ êµ¬ì„±í•˜ëŠ” ìš”ì†Œë³„ë¡œ ë‚˜ì—´í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

![Untitled](https://github.com/user-attachments/assets/02e07143-6c17-4cad-8052-df3732c57040)


(1) Agent

(2) í™˜ê²½
- Agentê°€ ìƒí˜¸ì‘ìš©í•˜ëŠ” í™˜ê²½
- í™˜ê²½ì€ Agentì—ê²Œ ìƒí™©(state)ë¥¼ ì¸í’‹ìœ¼ë¡œ ì£¼ê³  AgentëŠ” ì´ì— ëŒ€ì‘ë˜ëŠ” í–‰ë™(action)ì„ ì•„ì›ƒí’‹ìœ¼ë¡œ ë‚¸ë‹¤
- í™˜ê²½ì€ ë‹¤ì‹œ ì´ëŸ° actionì„ ë°›ì•„ì„œ Agentì—ê²Œ ìƒˆë¡œìš´ ìƒí™©(state)ì„ ë¦¬í„´ì‹œí‚´ (ì´ë¥¼ transitionì´ë¼ê³  í•¨)
- GANì— ëŒ€í•´ ìµìˆ™í•˜ë‹¤ë©´ ì‚¬ì‹¤ìƒ GANê³¼ ë¹„ìŠ·í•œ ì…‹ì—…ì´ë‹¤. ë‹¤ë§Œ GANì—ì„  í™˜ê²½ ì—­í• ì„ í•˜ëŠ” Discriminatorê°€ ë™ì‹œì— í•™ìŠµëœë‹¤ëŠ” ì 
- ì´ë•Œ stateì˜ ëª¨ë“  ì •ë³´ë¥¼ ì¤„ì§€ ë¶€ë¶„ë§Œ ì¤„ì§€ì— ë”°ë¼ : fullyÂ vs partiallyÂ observable

(3) policyÂ 
- mappingÂ fromÂ statesÂ toÂ actions.Â :Â bidÂ andÂ askÂ policy
- policy $\pi_{\theta}(a | s)$



(4) rewardÂ signal
- deviationÂ from

(5) valueÂ functions
- actionÂ valueÂ functionÂ forÂ policyÂ $\pi$
- stateÂ valueÂ functionÂ forÂ policyÂ $\pi$
(6) MDP
- ìœ„ì— ë§í•œ ì „ì²´ ìš”ì†Œë¥¼ ì¢€ ë” ìˆ˜í•™ì ì¸ ì–¸ì–´ë¡œ ëª…í™•í•˜ê²Œ ì •ë¦¬í•œ ê°œë…
- finiteÂ MarkovÂ DecisionÂ ProcessÂ (MDP) ê¸°ì¤€ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ë‹¤
    - aÂ finiteÂ setÂ ofÂ statesÂ $S$
    - b.Â finiteÂ setÂ ofÂ actionsÂ $A$
    - c. aÂ discountÂ rateÂ $\gammaÂ \inÂ [0,1]Â $
    - d. aÂ finiteÂ setÂ ofÂ rewardsÂ $R$
    - e. theÂ one-stepÂ dynamicsÂ ofÂ theÂ environemnt $\forall s,r',a,r :  p(s',r | s,a) = \Bbb{P} (S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$
    - AgnetÂ knowsÂ :Â a,b,c
    - AgentÂ doesÂ notÂ knowÂ :Â d,e
- AgentëŠ” aë¥¼ ì·¨í•˜ë©´ì„œ rë¥¼ ìµœëŒ€í™”í•˜ë ¤ê³  í•œë‹¤

- Transition Prob $P(s'|s,a)$ì´ ê³ ì • : Stationary Environment

## Dynamic Programming
- í° ë¬¸ì œë¥¼ ì‘ê²Œ ë‚˜ëˆ ì„œ í’€ê³  ì¡°ê¸ˆì”© í™•ì¥í•´ì„œ ë” í° ë¬¸ì œë¥¼ í‘¼ë‹¤
- constraintsì˜ ìµœì†Œ ë‹¨ìœ„ë¥¼ ì–´ë–»ê²Œ ì •ì˜í•˜ëŠëƒê°€ ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŒ
- ì‹œí€€ìŠ¤ ë§¤ì¹­ë„ DPë¡œ í‘¼ë‹¤
- ìµœì ì˜ ë¬¸ì œë¥¼ ë­”ê°€ ê³„ì‚°í•˜ê³  ì‹¶ì€ë° ë¬¸ì œê°€ ë„ˆë¬´ í¬ë©´ ì˜ê²Œ ìª¼ê°œì„œ í•´ê²° (Divide and conquer)


## DPì˜ í•œê³„ì 
1.Â ComputationÂ complexity

2.Â CurseÂ ofÂ dimension
- ì‘ì€ ê²©ìì˜ ë¯¸ë¡œ ì°¾ê¸°ë§Œ í•´ë„ state, action ê²½ìš°ì˜ ìˆ˜ê°€ í¬ë‹¤
- ì´ë¥¼ Q-tableë¡œ í•˜ëŠ” ê±´ ë¬´ë¦¬ -> Q-functionì„ ê·¼ì‚¬í™”
3.Â FullÂ descriptionÂ ofÂ environment
- í˜„ì‹¤ì—ì„  MDPë¼ëŠ”ê±¸ ì™„ë²½í•˜ê²Œ ì•Œ ìˆ˜ ì—†ê³  ì¸í„°ì•¡ì…˜ìœ¼ë¡œ ì•Œì•„ë‚´ì•¼ í•¨
- ì´ ë•Œë¬¸ì— 

## Exploration-Exploitation Dilema
- ë‹¨ê¸°ì ìœ¼ë¡œ ë³´ìƒì„ ì–»ëŠ” ì „ëµì„ ë§Œë“œëŠ” ê²ƒì€ ìƒëŒ€ì ìœ¼ë¡œ ì‰½ì§€ë§Œ ì¥ê¸°ì ì¸ ì „ëµì„ ë§Œë“œëŠ” ê±´ í˜ë“¤ ìˆ˜ ìˆë‹¤


# ê¸°ì´ˆ ì•Œê³ ë¦¬ì¦˜

ì „í†µì ì¸ Optimal planningìœ¼ë¡œëŠ” í˜ë“¤ë‹¤
ì •í™•í•˜ê¸´ í•˜ë‚˜ í•™ìŠµ íš¨ìœ¨ì„±ì´ ë‚®ë‹¤ -> $p(s_{t+1} | s_t, a_t)$ë¥¼ ì•Œì•„ì•¼ í•˜ê³  ì°¨ì›ì´ ì»¤ì§€ë©´ í˜ë“¬ (ëª¨ë“  stateì— ëŒ€í•´ iterationì„ ëŒë ¤ì•¼ í•´ì„œ)
$$
V_{k+1}(s)= \max_aâˆ‘_{s'}P(s'âˆ£s,a)[R(s,a)+Î³V_k(s')]
$$
- Vë¥¼ ë‹¤ ì°¾ê³ ë‚˜ì„œ Policyë¥¼ ì–»ìŒ
$$
V_{k+1}(s)= \max_aâˆ‘_{s'}P(sâ€²âˆ£s,a)[R(s,a)+Î³V(sâ€²)]
$$






ëŒ€ì‹ ì— ìƒ˜í”Œë§ ê¸°ë°˜ì˜ ê°•í™”í•™ìŠµ ë°©ë²•ì´ í•„ìš”í•¨


Policy Iteration
- policyÂ improvementÂ :Â withÂ $\epsilon$-greedyÂ policy.
- policyÂ evaluationÂ :Â evaluateÂ theÂ policyÂ usingÂ valueÂ function.
$$
\ \theta_{t+1} = \theta_{t} + \alpha \nabla_{\theta}J(\theta) \approx  \theta_{t} + \alpha [ \nabla_{\theta} \log{\pi_{\theta}(a|s)} q_{\pi}(s,a)]
$$

# ë¶„ë¥˜

- í•­ìƒ ë¶„ë¥˜ëŠ” ê¸°ì¤€ì ì„ ì–´ë–»ê²Œ ë‘ëŠëƒì— ë”°ë¼ ë‹¤ë¥´ê²Œ ë¶„ë¥˜í•  ìˆ˜ ìˆë‹¤.
- ë¨¼ì € ì—…ë°ì´íŠ¸ time-stepì— ë”°ë¼ 3 ê°€ì§€ í° ë¶„ë¥˜ë¥¼ í•  ìˆ˜ ìˆë‹¤
	- ìœ„ì—ì„œ ì‚´í´ë³¸ DP ë°©ì‹, MC ë°©ì‹, TD ë°©ì‹
	- ì•Œê³ ë¦¬ì¦˜ ì‹œê°„ì— ë°°ìš°ëŠ” DFS, BFSì˜ ê´€ê³„ì™€ ë¹„ìŠ·í•œ ì² í•™
	- leafê¹Œì§€ ì­‰ ë¨¼ì € ë‚´ë ¤ê°ˆ ê²ƒì¸ë‹ˆ ì•„ë‹ˆë©´ ì£¼ë³€ë¶€ë¥¼ íƒìƒ‰í•˜ê³  ê²°ì •í•œê²ƒì¸ì§€
- í™˜ê²½ì— ëŒ€í•´ ì•Œê³  ìˆëŠ”ì§€ì— ë”°ë¥¸ ë¶„ë¥˜
	- Model-free vs Model-based
- ì—…ë°ì´íŠ¸í•˜ê³ ì í•˜ëŠ” ëŒ€ìƒì— ë”°ë¥¸ ë¶„ë¥˜
	- Value-based vs Policy-based

![img1 daumcdn](https://spinningup.openai.com/en/latest/_images/rl_algorithms_9_15.svg)


- ë¨¼ì € í° ë¶„ë¥˜ë¡œëŠ” Model-free RL vs Model-based RL
	- agentê°€ í™˜ê²½ì´ë¼ëŠ” ëª¨ë¸ì˜ ì •ë³´ë¥¼ ì ‘ê·¼í•  ìˆ˜ ìˆëŠ”ì§€ ì—¬ë¶€ë¡œ êµ¬ë¶„
	- í™˜ê²½ : $s_{t+1}, r = env(s_t, a_t)$
	- ì—¬ê¸°ì„œ ëª¨ë¸ì´ë€ $p(s_{t+1} | s_t, a_t)$ ì´ë¼ê³  ë³´ë©´ ë¨
- Model-FreeëŠ” ë‚´ë¶€ì ì¸ ë™ì‘ ( inner working)ë³´ë‹¤ëŠ” ë¦¬ì›Œë“œë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•´ ìƒ˜í”Œë§ì„ ì‹ ê²½ì”€

$$
\pi_{\theta}(a_t | s_t) \\
\xcancel{p(s_{t+1} | s_t, a_t)}
$$
$$
\pi_{\theta}(a_t | s_t) \to \tau = \{ s_1, a_1, s_2, a_2, ..., s_T, a_T\} \\
\max_{\pi_{\theta}}\Bbb{E}_{\tau \sim \pi_{\theta}}[\sum_t \gamma^t r(s_t, a_t)] \\
$$

- ë°˜ë©´ Model-basedëŠ” ëª¨ë¸ì— ì§‘ì¤‘í•¨
$$
\xcancel{\pi_{\theta}(a_t | s_t)} \\
p(s_{t+1} | s_t, a_t) : \text{model}
$$
$$
\tau = \{ s_1, a_1, s_2, a_2, ..., s_T, a_T\} \\
\min_{a_1, ..., a_T} \sum_{t-1}^T c(s_t, a_t) : \text{cost} \\
 \text{such that} \quad s_t = f(s_{t-1}, a_{t-1}) : \text{model}
 $$

- í™˜ê²½ì— ëŒ€í•œ ì •ë³´ë¥¼ í†µí•´ planningì„ í•  ìˆ˜ ìˆì–´ì„œ sample efficiencyê°€ ë†’ë‹¤
- agentëŠ” planning ê²°ê³¼ë¥¼ í•™ìŠµí•œ policyì— ë°˜ì˜í•  ìˆ˜ ìˆìŒ
- ì•ŒíŒŒì œë¡œê°€ ëŒ€í‘œì ì¸ Model-basedì„
- ë¬¸ì œëŠ” biasê°€ ìƒê¸°ë©´ í•™ìŠµí•œ í™˜ê²½ì—ì„œë§Œ ì˜ ë™ì‘í•˜ê³  real envì—ì„  sub-optimalí•˜ê²Œ ë™ì‘
- Model-basedì‘ sampling efficienyê°€ ë‚®ì§€ë§Œ í•™ìŠµì´ ë” ì‰¬ì›€

- Model FreeëŠ” ë‹¤ì‹œ Policyê¸°ë°˜ ì ‘ê·¼ë²•ê³¼ Value ê¸°ë°˜(Q-Learning) ì ‘ê·¼ë²•ìœ¼ë¡œ í¬ê²Œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤
- Value-based

![Image](https://github.com/user-attachments/assets/b89f4d0a-7389-4e31-9cad-ea9fe12c640d)
- Value-basedì—ì„œë„ ì–´ì°Œ ë˜ì—ˆë“  Optimal Policyë¥¼ ì°¾ê²Œë¨

- Policy-based
![Image](https://github.com/user-attachments/assets/8e4edb79-14a3-4adf-81e7-c5a6356235cb)
	- optimal value functionì„ ì°¾ê¸° ì•Šê³  ë°”ë¡œ optimal policyë¥¼ ì°¾ëŠ”ë‹¤
	- ì¦‰ actionì„ ë°”ë¡œ ì•„ì›ƒí’‹ìœ¼ë¡œ ë‚´ë†“ëŠ” ë„¤íŠ¸ì›Œí¬ë¥¼ í•™ìŠµí•˜ì

![Image](https://github.com/user-attachments/assets/4bd932dd-4773-4363-bcfe-cb3a8862e707)
- actionë„ ì•„ì›ƒí’‹ì„ ì–´ë–»ê²Œ ì„¤ì •í•  ê²ƒì¸ì§€ ê³ ë¯¼í•  ìˆ˜ ìˆë‹¤. 
- softmaxë¥¼ ì¨ì„œ ê° action classì— ëŒ€í•œ í™•ë¥  ê°’ì„ ë‚´ëŠ” ì‹ìœ¼ë¡œ í•  ìˆ˜ ìˆë‹¤ : Stochastic policy
- argmaxë¥¼ ì¨ì„œ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ê²ƒë§Œ ì·¨í•˜ë„ë¡ í• ìˆ˜ë„ ìˆë‹¤ : Determinstic policy
- ë§Œì•½ categoricalí•˜ì§€ ì•Šê³  continuousí•˜ë‹¤ë©´ => tanhì‚¬ìš©í•¨

![Image](https://github.com/user-attachments/assets/920aa96a-cde8-4f49-a2c4-30a6f1848b5f)
- ê¸°ëŒ€ê°’ì„ ìµœëŒ€í™”í•˜ëŠ”ê²Œ ëª©í‘œì¸ë° ë„¤íŠ¸ì›Œí¬ weight $\theta$ ì°¾ëŠ”ê²Œ í¬ì¸íŠ¸
- $\theta$ê°€ ê²°êµ­ actionì„ ë§Œë“¤ê³  ì´ëŠ” ë‹¤ì‹œ rewardë¥¼ ì–»ëŠ” ê²ƒì´ë¯€ë¡œ ê¸°ëŒ€ê°’ì„ $\theta$ì˜ í•¨ìˆ˜ë¡œ í‘œí˜„ê°€ëŠ¥
## MC ë°©ì‹
- An episodeê°€ collectë˜ì–´ì•¼ Update(Episodic task ì‚¬ìš©)
- Returnì— ê¸°ë°˜í•˜ì—¬ ì—…ë°ì´íŠ¸

- ì´ˆê¸°ì—ëŠ” Equal probable random policyë¥¼ ê°€ì§ -> ëœë¤ í–‰ë™
- í™˜ê²½ì€ ì´ëŸ¬í•œ ëœë¤ í–‰ë™ì„ í•˜ëŠ” Agentì— next_stateì™€ rewardë¼ëŠ” ì •ë³´ë¥¼ ì£¼ê²Œ ë¨
- ì—…ë°ì´íŠ¸ë¥¼ ê±°ì³ì„œ $\pi_{\theta}^{*}$ë¥¼ ì–»ê²Œ ë¨

### MC Prediction

### MC Control
![Image](https://github.com/user-attachments/assets/5650b5a2-52ba-4d71-8297-b20b18defb0d)
- Policyë¥¼ ìµœì í™”í•¨
- ë‘ ê°€ì§€ ê³¼ì • : Policy Evaluation + Policy Improvment
	- í˜„ì¬ì˜ policy $\pi$ë¥¼ ì´ìš©í•´ì„œ episode $(S_0, A_0, R_1, \dots, S_T)$ë¥¼ ëª¨ìŒ
	- $Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha(G_t - Q(S_t, A_t))$
	- $Q(S_t, A_t)$ì—ëŠ” expected returnì— ëŒ€í•´ agentê°€ ì¶”ì •í•œ ì •ë³´ê°€ ë°˜ì˜ë˜ì–´ ìˆìŒ
	- ë§Œì•½ return $G_t \neq Q(S_t, A_t)$ ë¼ë©´ ì—…ë°ì´íŠ¸í•˜ê²Œ ë¨
- êµ¬ì²´ì ìœ¼ë¡  3ê°€ì§€ ë°©ë²•
	- Incremental mean
	- Constant-alpha
	- Constant $\alpha$ MC Control



## TD
- í•˜ë‚˜ì˜ ìŠ¤í…ì„ ë³´ê³  ì—…ë°ì´íŠ¸ ìˆ˜í–‰
- ì—…ë°ì´íŠ¸ ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŒ
$$
V(s_t)â†V(s_t)+Î±[r_t+Î³V(s_{t+1})âˆ’V(s_t)]
$$


## N-step bootstrapping
![Image](https://github.com/user-attachments/assets/6b4f2b18-1f87-438b-9e44-2b817af0c60d)

- TDì™€ MCì˜ ì¤‘ê°„
- TDë¥¼ ì‚¬ì‹¤ 1-step bootstrappingì´ë¼ê³  ë³¼ ìˆ˜ ìˆìŒ
- ì¢€ ë” í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ì—¬ ê²½í—˜ì„ ìŒ“ê¸° ë•Œë¬¸ì— Biasë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤

## ìˆ˜ë ´ ì¡°ê±´
- TDì˜ ê²½ìš° MC, DPì™€ ë‹¬ë¦¬ ì´ì „ ê²½í—˜ì„ ê¸°ë°˜ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ê¸° ë–„ë¬¸ì— learning rateë¥¼ ì ì ˆí•˜ê²Œ ì¡°ì ˆí•´ì•¼í•¨
- ì´ë¡ ì ìœ¼ë¡œ ìˆ˜ë ´í•˜ë ¤ë©´ ë‹¤ìŒ ì¡°ê±´ ë§Œì¡±
(1) Step Size í•™ìŠµë¥  ì¡°ê±´
$$
Q(s,a)â†Q(s,a)+Î±[targetâˆ’Q(s,a)]
$$

$$
âˆ‘_{t=1}^âˆÎ±_t=âˆ, âˆ‘^âˆ_{t=1}Î±_t^2<âˆ
$$

(2) GLIE (Greedy in the Limit with Infinite Exploration) ì¡°ê±´
- ì¶©ë¶„íˆ ëª¨ë“  stateë¥¼ ê²½í—˜í•´ë´ì•¼í•¨
- ëª¨ë“  ìƒíƒœ-í–‰ë™ ìŒì´ ë¬´í•œ ë²ˆ ë°©ë¬¸ë˜ì–´ì•¼ í•¨.
- ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ $ğœ–â†’0$ ì ì  greedy

- ê·¸ë ‡ì§€ë§Œ í˜„ì‹¤ì—ì„  GLIEì„ ë¬´ì‹œí•˜ê³  ğœ– ì„ ì ì  ì¤„ì´ì§€ ì•Šê³  ê³ ì •ëœ ê°’(ì˜ˆ: 0.1)ìœ¼ë¡œ ìœ ì§€
- ë˜í•œ Adam ê°™ì´ ê³ ì •ëœ í•™ìŠµë¥ ì„ ì‚¬ìš©

### TD Control : SARSA and SARSA-Max(Q-learning)

$ Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha(R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) )$

- $R_{t+1} + \gamma Q(S_{t+1}, A_{t+1})$ : InterpreteÂ asÂ target (TD target, Bellman eq RHS)
- $Q(S_t, A_t)$ : InterpreteÂ asÂ prediction (Bellman eq LHS)

- ì¸ê³µì‹ ê²½ë§ì˜ ê²½ìš° SGDë¥¼ ëŒë ¤ì•¼ í•˜ë¯€ë¡œ MSEë¡œ ìƒê°
$ \text{MSE} = (y-\hat{y})^2 = ( R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t))^2$


SARSAë¥¼ êµ¬í˜„í•˜ëŠ” ë°©ì‹ì´ 2ê°€ì§€ì´ë‹¤ : on-policy & off-policy

### (1) on-policy
- ì‹¤ì œí•  í–‰ë™ì— ëŒ€í•œ aciton-value ì¶”ì •ì¹˜ë¥¼ $Q(s', a')$ì— ì‚¬ìš©
- ê°€ì¥ ìµœê·¼ ë²„ì „ì˜ policyë¥¼ ë”°ë¼ì„œ í–‰ë™í•˜ë©´ì„œ ìˆ˜í–‰í•œ ë°ì´í„°ë§Œìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ëŠ” ê²ƒ
- epsilon-greedyë¡œ í•˜ê²Œëœë‹¤ë©´ Explorative action(a) + Explorative estimation(a')
- íƒ€ê²Ÿì´ ë˜ëŠ”ê²ƒ ê¹Œì§€ ê°™ì´ ë°°ì›€ -> ë¶ˆì•ˆì • -> sampleê°„ì˜Â correlationì´Â ë†’ë‹¤ê³  í‘œí˜„


### (2) SARASA Max(Q-learning)
$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha(R_{t+1} + \gamma \max_{a'}Q(S_{t+1}, a') - Q(S_t, A_t) )
$$

- ë°”ë¡œ ìµœëŒ€ action-valueë¥¼ ë‚´ëŠ” action ë“¤ ì¤‘ì—ì„œ ìµœëŒ€ê°’ì„ ë‚´ëŠ”ê²ƒì„ ê³¨ë¼ë²„ë¦¼ : ìˆ˜ì§‘ì´ í•„ìš”í•œ ìƒ˜í”Œì€ $\{ s,a,r,s' \}$

- í•™ìŠµ ë„ì¤‘ ì•„ë¬´ ì‹œì ì— ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ì´ìš©í•´ì„œ q-functionì„ ì—…ë°ì´íŠ¸
- í–‰ë™í•˜ëŠ”Â ì •ì±…ê³¼Â ë…ë¦½ì ìœ¼ë¡œÂ í•™ìŠµÂ :Â í–‰ë™í•˜ëŠ”Â ì •ì±…ê³¼Â í•™ìŠµí•˜ëŠ”Â ì •ì±…ì„Â ë¶„ë¦¬
- SARSA ëŒ€ë¹„ ì¢‹ì€ ì  : $a'$ë¥¼ ëœë¤í•˜ê²Œ ì„ íƒí•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ ì´ì „ì— ì¢‹ë‹¤ê³  íŒë‹¨í•œ ê³³ì„ ê° -> ë¬¸ì œê°€ ë˜ëŠ” stateë¡œ ê°€ëŠ” Që¥¼ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒ.
- s'ì—ì„œ ê°€ì¥ ìµœëŒ€ê°’ì„ ì£¼ëŠ” ê²ƒì„ íƒí•¨
- Explorative action(a) + Confident estimation(a')
- Rì´ ì˜ ì£¼ì–´ì§ˆ ë•ŒëŠ” ì¶”ì •ì¹˜ê°€ ì˜ ë§ìœ¼ë¯€ë¡œ ì´ê²Œ ë” íš¨ìœ¨ì 
- ì´ˆë°˜ Over-confident -> Risky
- ì•ŒíŒŒê³ 1ì´ ì´ ë°©ì‹ìœ¼ë¡œ í–ˆëŠ”ë° í‹°ì²˜ë¥¼ ì˜ ì…‹íŒ…í•´ë‘ì—ˆê¸° ë•Œë¬¸ì— ì´ ë°©ì‹ì´ ë¬¸ì œê°€ ë˜ì§€ ì•ŠìŒ

- ì´ë¥¼ Neural Netìœ¼ë¡œ êµ¬í˜„í•œê²Œ DQNì´ë‹¤. 
	- ì¶”ê°€ì ì¸ íŠ¸ë¦­ìœ¼ë¡œ Replay Bufferë¥¼ ì‚¬ìš©í•¨
	- ì´ëŠ” ë ˆì–´í•œ ì‹œí€€ìŠ¤ê°€ ìœ ìš©í•  ìˆ˜ ìˆì§€ë§Œ ëœ ì—…ë°ì´íŠ¸ ë˜ëŠ”ê²ƒì„ ë°©ì§€í•  ìˆ˜ ìˆìŒ
	- Action $A_{t}$ì™€ ë‹¤ìŒ State $S_{t+1}$ëŠ” correlatedë˜ì–´ ìˆë‹¤ê³  í‘œí˜„í•¨ 
	- ì´í›„ Replay Bufferì—ì„œ ëœë¤í•˜ê²Œ ë½‘ì•„ëƒ„
	- ë§Œì•½ ë‹¤ ì°¼ìœ¼ë©´ ì˜›ë‚ ê±° ë¶€í„° ì‚­ì œ
	- ì´ì „ Policyì—ì„œ ë§Œë“¬ ìƒ˜í”Œì„ ì´ìš© -> í˜„ì¬ í•™ìŠµí•˜ëŠ” Policyì™€ ë‹¤ë¦„

FromÂ Q-functionÂ upateÂ :
$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma \max_{a'}{Q(S_{t+1}, a')} - Q(S_t, A_t))
$$

WeÂ canÂ deriveÂ lossÂ functionÂ for DQN:
$$
MSE = (R_{t+1} + \gamma \max_{a'}{Q(s', a', \theta)} - Q(s', a', \theta))^2
$$
ProblemÂ withÂ bootstrapÂ :Â targetÂ forÂ updateÂ keepsÂ chaning.Â ToÂ preventÂ this,Â weÂ needÂ fixedÂ networkÂ forÂ someÂ duration.
$$
\therefore MSE = (R_{t+1} + \gamma \max_{a'}{Q(s', a', \theta^{-})} - Q(s', a', \theta))^2
$$

- ì´ë¥¼ í†µí•´ optimal action-value function $Q^{*}(s, a)$ë¥¼ ê·¼ì‚¬í™”

- í–‰ë™í•˜ëŠ”Â ì„ íƒì€Â $\epsilon$Â greedyÂ policyë¥¼Â ì‚¬ìš©í•´ì„œÂ ìƒ˜í”Œì„Â ë§Œë“¤ì–´ëƒ„.
- ì—…ë°ì´íŠ¸ëŠ”Â BellmanÂ OptimalÂ Equationì„Â ì‚¬ìš©í•´ì„œ

$$
Q*(s,a) = \Bbb{E} [R_{t+1} + \gamma \max_{a'} Q*(S_{t+1}, a') | S_t = s, A_t = a]
$$



## C51
- distributed RL
- rewardê°€ í•˜ë‚˜ê°€ ì•„ë‹˜ -> ë¶„í¬ë¡œ ìƒê° (ë¶ˆí™•ì‹¤ì„± ë°˜ì˜)


# ë”¥ëŸ¬ë‹ ë°©ì‹
## Value-based
- approximate value function : $V_pi(s), Q_pi(s,a), A_pi(s,a)$


### Deep-SARSA
- Q-functionì„Â NNìœ¼ë¡œÂ ê·¼ì‚¬
- í•™ìŠµì„ ë¬¼ë¡  SGDë¥¼ ì‚¬ìš©

### DQN

### Double DQN
- Targetì„ ë‚´ëŠ” ë„¤íŠ¸ì›Œí¬ ìì²´ë„ í•™ìŠµí•˜ë©´ ë¶ˆì•ˆì •í•¨ (ë°°ê°€ ì‚°ìœ¼ë¡œ...)
- ì•„ì˜ˆ ë„¤íŠ¸ì›Œí¬ë¥¼ ë¶„ë¦¬í•´ì„œ cross-referenceí•¨.


- Fixed Q-target
- priotized experience replay
- dueling networks


### Exploration-Exploitation Trade-off
- Greedy Policy

- Epsilon Greedy Policy
$$
\pi(a|s) \begin{cases} 1-\epsilon + \frac{\epsilon}{|\mathcal{A}(s)|} & \text{if} \quad a \quad \text{maximizes} \quad Q(s,a) \\ \frac{\epsilon}{|\mathcal{A}(s)|} & \text{else} \end{cases}
$$



## Policy-based
- (s,a)ì—ì„œ rì„ ì£¼ëŠ” ìƒí™©ì´ ê´€ì‹¬ì´ ì—†ê³  ê·¸ëƒ¥ aë§Œ ì¤‘ìš”í•œ ê²½ìš°
- $\pi_{\theta}(s,a)$ ì§ì ‘ ê·¼ì‚¬í™”í•˜ì

```python
def bulid_model(self):
  model = Sequential()
  model.add(Dense(24, input_dim=self.state_size, activation='relu'))
  model.add(Dense(24, activation='relu'))
  model.add(Dense(self.action_size, activation='softmax'))
  return model
```


- ì‹ ê²½ë§ í•™ìŠµ ë…¼ë¦¬ë§Œ ì¶©ì‹¤íˆ ë”°ë¥´ì
- $\Bbb{E}[r]$ ë¥¼ ìµœëŒ€í™”í•˜ë„ë¡ $\pi_{\theta}(s,a)$ë¥¼ í•™ìŠµ
- í™•ë¥ ê°’ì´ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì— Greedy Policy, Epsilon Greedy Policy ê°™ì€ê²Œ ë‚´ì¬ë˜ì–´ ìˆìŒ

- ê·¸ë¦¬ê³  í™•ë¥ ì ìœ¼ë¡œ actionì„ ì„ íƒí•´ì„œ local minimaì— ë¹ ì§€ì§€ ì•Šê²Œ
- future rewardì— ëŒ€í•œ ì‹ì„ objectiveë¡œ ì¡ìŒ
ë¯¸ë¶„í•´ì„œ ë³´ë‹ˆê¹ Q ë„¤íŠ¸ì›Œí¬ê°€ ë‹¤ì‹œ í•„ìš”í•˜ë”ë¼

- Objective function Jë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ëª¨ë¸ë§í•˜ê³  gradientë¥¼ ì°¾ì.

$$
J(\theta) = \Bbb{E}_{\tau \sim \pi_{\theta}}[R(\tau)] \\
\nabla_{\theta}J(\theta) = \Bbb{E}_{\tau \sim \pi_{\theta}}[\sum_{t=0}^T \nabla_{\theta} \log{\pi_{\theta} (a_t | s_t)} R(\tau)] \\
\nabla_{\theta}J(\theta) = \Bbb{E}_{\tau \sim \pi_{\theta}}[\sum_{t=0}^T \nabla_{\theta} \log{\pi_{\theta} (a_t | s_t)} \sum_{t'=t}^T R(s_{t'},a_{t'},s_{t'+1} )]
$$
- ì œì¼ ì•ˆìª½ sumì— ìˆëŠ” rewardì˜ í•©ì„ reward-to-goÂ policyÂ gradien
$$
\hat{R_t} = \sum_{t'=t}^T R(s_{t'},a_{t'},s_{t'+1} )
$$


f_wê°€ valu functionì¸ë° 

Reward $G$ì™€ $J$ì˜ ì°¨ì´
	- 
GëŠ” single episodeì—ì„œ ëª¨ì´ëŠ” returnì´ê³  JëŠ” expected returnì´ë‹¤. 
ì‹¤ì œ thetaê°€ ë˜‘ê°™ë”ë¼ë„ í™˜ê²½ì—ì„œ ì˜¤ëŠ” ëœë¤ì„±ê³¼ policyê°€ stochasticí•˜ë‹¤ë©´ 
ì—í”¼ì†Œë“œê°€ ëë‚˜ê³  ì–»ì–´ì§€ëŠ” return GëŠ” ì¼ì •í•˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.
ë”°ë¼ì„œ expected return Jì˜ ì •í™•í•œ ì¶”ì •ê°’ì€ ì•„ë‹˜ ì‹¤ì „ì—ì„  ì¶©ë¶„íˆ ì¢‹ë‹¤

- Hill climbing, Simulated Annealing(Random restarts) ë“± ê¸°ì¡´ì˜ ì•Œê³ ë¦¬ì¦˜ ë˜í•œ Policy ë°©ì‹ìœ¼ë¡œ
ìƒê°í•  ìˆ˜ ìˆìŒ
- ê¸°ì¡´ì˜ Black-box optimization ë°©ë²•ë¡ ë“¤ë„ Policyì˜ $\theta$ë¥¼ ì°¾ëŠ” ì‹ìœ¼ë¡œ ì ìš© ê°€ëŠ¥ (Hill-climbing like, Cross-Entropy method, Evolution Strategy)


- ëŒ€ì‹  ë”¥ëŸ¬ë‹ í•™ìŠµì„ ì´ìš©í•  ê²ƒì´ë¯€ë¡œ Policy gradient methodì— ì£¼ëª©í•¨


### Stochastic PG vs Deterministic PG
- $\pi_{\theta}(s, a)$ vs $a=\mu_{\theta}(s)$
- ìš´ì „ìƒí™© : ì´ëŸ°ê±°ì €ëŸ°ê±° ë³´ë‹¨ compactí•˜ê²Œ actionì„ ì·¨í•´ì•¼í•¨
- ì‹ ê²½ë§ì˜ ì•„ì›ƒí’‹ì´ continuos ê°’ì„ ëƒ„ => continuous action space êµ¬í˜„ ê°€ëŠ¥


Policy gradientëŠ” Supervised learningê³¼ ë¹„ìŠ·í•¨



### Deterministic PG
- David Silver(ì•ŒíŒŒê³  ì œì‘ ì´ê´„ì)
- Q function : í˜„ì¬ ìƒí™©ì´ ì–¼ë§ˆì ì¢‹ì€ì§€ë¥¼ íŒë‹¨í•¨ => criticì´ë¼ê³  ë¶€ë¦„
- Actor : aë¥¼ ë‚´ë†“ìŒ


### Vanila Policy Gradient



### REINFORCE
![Image](https://github.com/user-attachments/assets/2c22c1bb-4cd1-45fd-a180-1ae0e1cfda5c)
- Trajectory $\tau = \{s_0, a_0, s_1, a_1, \dots, s_H, a_H, s_{H+1} \}$
- $R(\tau) = r_1 + r_2 + \dots + r_H + r_{H+1}$ : íŠ¹ì • trajectoryì˜ ë¦¬í„´
- expected return $U(\theta)$ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ trajectoryë¥¼ $\theta$ë¡œ ë¶€í„° ìƒ˜í”Œë§í•˜ì—¬ gradientë¥¼ êµ¬í•´ì„œ ìµœì í™”í•˜ì
- í•´ë‹¹ trajectoryê°€ ë‚˜ì˜¬ í™•ë¥  $P(\tau ; \theta)$ëŠ” policyì—ì„œ ë‚˜ì˜¤ëŠ” í™•ë¥ ì„ ê³±ìœ¼ë¡œ í‘œí˜„í•˜ê³  ì´ë¥¼ ë‹¤ì‹œ logë¥¼ ì·¨í•˜ë©´ í•©ìœ¼ë¡œ í‘œí˜„ë¨ : $âˆ‘_{t=0}^T R_tâˆ‡_Î¸logÏ€_Î¸(a_tâˆ£s_t)$


- ë‹¨ì 
	- (1) updateê°€ ë¹„íš¨ìœ¨ì  : policyì—ì„œ trajectory ë½‘ê³  ë‹¤ì‹œ ì•ˆì”€
	- (2) noisy gradient : í™•ë¥ ì ìœ¼ë¡œ actionì„ ì·¨í•˜ê¸° ë•Œë¬¸ì— ìƒ˜í”Œë§í•œ trajectoryê°€ ì‹¤ì œ policy $\pi$ë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•  ìˆ˜ë„
	- (3) unclear credit assignment : total returnë§Œ ë³´ê¸°ë•Œë¬¸ì— bad actionì´ í¬í•¨ë ìˆ˜ ìˆìŒ
	- (4) Returnì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ì—í”¼ì†Œë“œê°€ ëë‚˜ì•¼ í•™ìŠµê°€ëŠ¥ 


Monte-Carlo sampling

sampling complexity efficiency

noisy varaicne gradient

â€˜goddâ€™ and â€˜bacâ€™ action â†’ variance

culumative returns +  variablity of trajectories â†’ gradient

Maximize Expected Return

### [TRPO 15.02](https://arxiv.org/abs/1502.05477)
- PGì—ì„œ ì•ˆì •ì„±ì„ ë†’ì¸ ì•Œê³ ë¦¬ì¦˜
- step sizeê°€ ë„ˆë¬´ í¬ë©´ ì—°ì‡„ì ì¸ ë¬¸ì œì ì´ ìƒê¹€ : Bad policy â†’ Bad action â†’ No signal â†’ Thus terrible policy
- line searchë¥¼ í†µí•´ ìµœì ì˜ step sizeë¥¼ ì°¾ìŒ (ë¹„ìš©ì€ ì¦ê°€ë¨ë¨)
- trust regiondì´ë¼ëŠ” ê°œë…ì„ í†µí•´ ê¸°ì¡´ Policyì—ì„œ ë„ˆë¬´ ë²—ì–´ë‚˜ì§€ ì•ŠëŠ” regularizationì„ ë„ì…
$$L^{TRPO}(Î¸)=E[ Ï€ Î¸ old(aâˆ£s)Ï€ Î¸(aâˆ£s)A Ï€ Î¸ old(s,a)] 
$$
- TRPO is powerful but computationally expensive due to second-order optimization.
- Trust Regionì„ ìœ ì§€í•˜ë ¤ê³  Second-Order Optimization(Natural Gradient)ì„ ì‚¬ìš©
- í•˜ì§€ë§Œ ì´ëŸ° trust region constraintì„ ë¶€ì—¬í•˜ëŠ”ê²Œ ì‰½ì§€ ì•ŠìŒìŒ



PPO removes the need for second-order optimization by using a clipped surrogate loss.
PPO is more scalable and can be used with first-order optimizers like Adam.
Clipping in PPO replaces the KL divergence constraint in TRPO, making it easier to implement and more efficient.
PPO_v2 : Clipped Surrogate Loss


### [PPO](https://arxiv.org/abs/1707.0634)
- REINFORCEì˜ ë‹¨ì ì„ ê°œì„ 
- TRPOì˜ ë‹¨ì ì„ ê°œì„ 
- performanceë¥¼ ìµœëŒ€í™”í•˜ë©´ì„œ ê°„ì ‘ì ìœ¼ë¡œ policyë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤
    - surrogate objective function
    - estimate of $J(\pi_{\theta})$

- í•˜ë‚˜ì˜ trajectoryì— ì˜¤ëŠ” ë…¸ì´ì¦ˆë¥¼ ë‚®ì¶”ê¸° ìœ„í•´ ë” ë§ì€ trajectoryë¥¼ ìƒ˜í”Œë§í•¨ (ë³‘ë ¬ì²˜ë¦¬í•˜ë©´ ë” íš¨ìœ¨ì )

1.Â First,Â collectÂ someÂ trajectoriesÂ basedÂ onÂ someÂ policyÂ $\pi_{\theta}$,Â andÂ initializeÂ thetaÂ primeÂ $\theta'Â =Â \theta$

2.Â Next,Â computeÂ theÂ gradientÂ ofÂ theÂ clippedÂ surrogateÂ functionÂ usingÂ theÂ trajectories

3.Â UpdateÂ $\theta'$usingÂ gradientÂ ascentÂ $\theta'\leftarrowÂ \theta'Â +Â \alphaÂ \nabla_{\theta'}Â L_{sur}^{clip}Â (\theta',Â \theta)$

4.Â ThenÂ weÂ repeatÂ stepÂ 2-3Â withoutÂ generatingÂ newÂ trajectories.Â Typically,Â stepÂ 2-3Â areÂ onlyÂ repeated aÂ fewÂ times

5.Â SetÂ $\thetaÂ =Â \theta'$,Â goÂ backÂ toÂ stepÂ 1,Â repeat.



### TD3


## Actor-Critic
![Image](https://github.com/user-attachments/assets/c148b185-dbf7-40d0-bbde-74f972a68f04)

- Value-based + Policy-based Hybrid
- Value-based (DQN)ì€ ë³´í†µ 3ê°€ì§€ë¥¼ ê·¼ì‚¬í• ê³  í•¨ (state-value function, action-value function, advantage function)
- Policy-basedì—ì„  Stochastic policyì˜ ê²½ìš° í™•ë¥ ì€ Determinstic policyì˜ ê²½ìš° actionì„ ë‚´ë†“ìŒ.

![Image](https://github.com/user-attachments/assets/d4bb3ce0-d739-4c2d-a035-2f0eaf828059)

![Image](https://github.com/user-attachments/assets/04b288eb-3640-4daf-813a-23f5a41e6c70)

- Criticì´ baselineì„ ì œê³µí•˜ë©´ì„œ boostrappingì„ í• ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤ -> Policy-basedì˜ varianceë¥¼ ì¤„ì´ì.
- ê¸°ê³„í•™ìŠµì—ì„œ í•­ìƒ ë‚˜ì˜¤ë“¯ì´ bias-variance trade-offê°€ ìˆëŠ”ë° íŠ¹íˆ ê°•í™”í•™ìŠµì—ì„  Returnì„ ê³„ì‚°í•˜ëŠ”ë° ë°œìƒ

- Actor = Policy netowrk ê·¼ì‚¬í™”
	- action_probë¥¼Â ë‚´ë†“ìŒ
	-  TD error ($\delta_v$) is not function of $\theta$
$$
\theta_{t+1} \approx \theta_{t} + \alpha [\nabla_{\theta}\log \pi(a|s) \delta_v]
\to
\theta_{t+1} \approx \theta_{t} + \alpha\nabla_{\theta} [\log \pi(a|s) \delta_v]
$$
- Critic = Q-function ê·¼ì‚¬í™”í™”
	- Actorì˜ í–‰ë™ì´ ì¢‹ì€ì§€ë¥¼ íŒë‹¨
$$
\ \theta_{t+1} \approx  \theta_{t} + \alpha [ \nabla_{\theta} \log{\pi_{\theta}(a|s)} Q_w(s,a)]
$$
$$
\text{MSE} = (\text{target} - \text{prediction} )^2 = (R_{t+1} + \gamma V_v(S_{t+1}) - V_v(S_t))^2
$$

### A3C (Asynchronous Advantage Actor-Critic)
- gradient ascentë¥¼ ìˆ˜í–‰í•˜ì—¬ ì§ì ‘ performanceë¥¼ ìµœëŒ€í™”
- í•˜ë‚˜ì˜ ë„¤íŠ¸ì›Œí¬ weightë¥¼ Actorì™€ Criticì´ ê³µìœ í•œë‹¤.



### A2C (Advantage Actor-Critic)
- A3C(Asynchronous Advantage Actor-Critic)ì—ì„œ ë™ê¸°í™” ë²„ì „(Synchronous)ìœ¼ë¡œ ë³€í˜•ëœ ì•Œê³ ë¦¬ì¦˜




### DDPG
- Policy gradient theoremì— ì˜í•´ Që¥¼ êµ¬í•´ì„œ $\pi$ ì—…ë°ì´íŠ¸ ìˆ˜ì‹ì— ëŒ€ì…
- off-policy actor-critic
- target network
- ActionNet : $S \to A$
```python
class Actor(nn.Module)
	__init__(cfg)
		super().__init__()
		# [state_size] -> [hidden_size] -> [action_size]
		self.model = ...
	forward(state)
		x = self.model(state)
		return F.tanh(x)

class Critic(nn.Module)
	__init__(cfg)
		# [state_size] -> [hidden_size] + [action_size] -> [hidden_size] -> [1]
	forward(state, action)
		x = self.model1(state)
		x = torch.cat((x, action), dim=-1)
		self.model2(x)
		
	
```
```python
for i in range(num_episodeds):
	env.reset()
	agent.reset()

	state = env.initial_state()
	score = np.zeros(num_agents)
	for t in range(timestep):
		action = agent(state)
		next_state, rewards, done = env.step(action)
		agent.step(state, action, reward, next_state, done)
		state = next_state
		score += reward

		if np.any(dones):
			break

	scores.append(np.mean(score))
	if i % CKT_FREQ == 0 or i == num_episodes:
		save()
	
```

### [D4PG, 18.04](https://arxiv.org/abs/1804.08617)
- ê¸°ëŒ€ê°’ë³´ë‹¨ ë¦¬ì›Œë“œì˜ ë¶„í¬ë¥¼ ëª¨ë¸ë§
- ë¶„ì‚° í•™ìŠµ ì ‘ê·¼ë²• : Critic í•™ìŠµì„ ë” í–¥ìƒ + Replay bufferë¥¼ Actorë¼ë¦¬ ê³µìœ 



### SAC
- stochasticÂ policyÂ optimizationÂ + DDPG-styleÂ approaches(clippedÂ doubleÂ Q functionÂ +Â targetÂ policyÂ smoothing)
- off-policy
- entropyÂ regularizationÂ term
	- $H(P(x)) = E_{x \sim p} [-logP(x)]$
	- maximize the entropy of policy
	- expcetedÂ return(exploitation)Â vs entropyÂ (randomnessÂ inÂ theÂ policy,Â exploration)
- Policy network $\pi_{\theta}$, Q-function : $\phi_1, \phi_2$, replay buffer : $D$
- target Q-function : $\phi_{1, t} \leftarrow \phi_1, \phi_{2,t} \leftarrow \phi_2$



# Multi-agent(MARL)
- ììœ¨ì£¼í–‰ ìë™ì°¨(SDC)ì˜ ìƒí™©ì„ ìƒê°í•˜ë©´ ë‹¤ë¥¸ ìë™ì°¨ë¥¼ Agentë¼ê³  ë³¼ ìˆ˜ ìˆìŒ
- ê°€ì¥ í˜„ì‹¤ì„¸ê³„ì— ê°€ê¹Œìš´ ì…‹íŒ…ì´ ë°”ë¡œ MA ìƒí™©
- ì„œë¡œ í˜‘ë™(Cooperative)ì„ í•˜ê±°ë‚˜ ê²½ìŸ(Competitive), ë³µí•©ì ì¸ ìƒí™©(Mixed Environments)ì¼ìˆ˜ë„
- ì¥ì ì„ ë­˜ê¹Œ?
	- ë§Œì•½ ë‹¤ë¥¸ ìë™ì°¨ë“¤ì˜ experienceë¡œ ëª¨ì•„ì„œ í•™ìŠµì„ í•  ìˆ˜ ìˆë‹¤ë©´ ë” í’€ìŠ¤í™ìŠ¤íŠ¸ëŸ¼ì„ í•™ìŠµí•  ë“¯
- Agnetë¼ë¦¬ ìƒí˜¸ì‘ìš©ì„ í•˜ë©´ì„œ ìƒˆë¡œìš´ í–‰ë™ê³¼ ë³µì¡ë„ ë°œìƒ
- Markov Game Framework
$ <n, S, A_1, \dots, A_n, O_1, \dots, O_n, R_1, \dots, R_n, \pi_1, \dots, \pi_n, T> $
- $n$ : # of agents
- $S$ : set of 
- $A$ : $A_1 \times A_2 \dots \times A_n $
- $R_i$ : $ S \times A \to R$ (ê° agent iì— ëŒ€í•œ reward)
- $\pi_i$ : $O_i \to A_i$ $ (ê° agent iì— ëŒ€í•œ policy)
- $T$ : $S \times A \to S$ (State transition function)
- State transition ë˜í•œ Markovian ê°€ì • (Next stateëŠ” ì˜¤ì§ present stateì™€ ì—¬ê¸°ì„œ ì·¨í•˜ëŠ” Joint actionì— ì˜í•´ì„œë§Œ ê²°ì •)
- 3ê°€ì§€ ì¡°ê±´
	- (1)learned policies can only use local information (i.e. their own observations) at execution time
	- (2)do not assume a differentaible model of environment dynamics
	- (3)No particular structure on the communication method between agents

- ì „í†µì ì¸ RLì€ MA í™˜ê²½ì— ì í•©í•˜ì§€ ì•ŠìŒ
	- Q-learning and DQN
		- inherent non-stationary ë¬¸ì œ : ê°ê° agent iê°€ ê°œë³„ì ì¸ optiaml Q_ië¥¼ í•™ìŠµ
		- í•˜ì§€ë§Œ ê°œë³„ì ìœ¼ë¡œ policyë¥¼ ë”°ë¡œ ì—…ë°ì´íŠ¸í•˜ê¸° ë•Œë¬¸ì— ê°œë³„ì ì¸ agentì˜ ì‹œì ì—ì„œ environmentê°€ non-stationaryí•˜ê²Œ ë³´ì„
    	- ì´ë¥¼ Q-learningì˜ ìˆ˜ë ´í•˜ê²Œ ìœ„í•œ Markov assumptionì„ ìœ„ë°°í•¨
	- Policy gradient
		- PGëŠ” high variance gradient estimates
		- agents ìˆ«ìê°€ ëŠ˜ìˆ˜ë¡ varianceê°€ ì»¤ì§
		- í•œ agentì˜ rewardëŠ” ë³´í†µ ë‹¤ë¥¸ agentsì˜ actionì— ì˜ì¡´í•˜ê¸° ë•Œë¬¸
		- agent ìì‹ ë§Œì˜ actionìœ¼ë¡œë§Œ conditionedëœ rewardëŠ” variabiltyê°€ ì‹¬í•˜ê³  gradientsì˜ varianceë¥¼ ì¦ê°€ì‹œí‚´
    	- ì™œëƒë©´ optimization ê³¼ì •ì¤‘ì— ë‹¤ë¥¸ agentsì˜ actionì´ ë°˜ì˜ë˜ì§€ ì•Šì•„ì„œ
		- baseline(value function baseline)ì„ ì¨ì„œ high varianceë¥¼ ì™„í™”í•˜ë ¤ê³  í•˜ì§€ë§Œ MA í™˜ê²½ì—ì„  non-stationary ì´ìŠˆê°€ ìƒê¹€
## í•™ìŠµ
- Single-agent RL algorithm vs Meta Agent Approach
- Either leads to non-stationarity or Large joint action space

### [MADDPG](https://arxiv.org/abs/1706.02275)
- DDPGë¥¼ MAê¸°ë°˜ìœ¼ë¡œ í™•ì¥
- local information(their own observation)ë§Œ ì¨ì„œ policyí•™ìŠµ
- environment dynamicsì˜ ëª¨ë¸ì´ differentiableí•˜ì§€ ì•Šì•„ë„ ë¨
- agentsê°„ì˜ íŠ¹ë³„í•œ ì˜ì‚¬ì†Œí†µì— íŠ¹ë³„í•œ êµ¬ì¡°ë¥¼ ê°€ì •í•˜ì§€ ì•Šì•„ë„ ë¨
- centralized training with decentralized execution
	- ë©”ì¸ Criticì„ í•˜ë‚˜ ë‘ê³  Actorë¥¼ ê°œë³„ì ìœ¼ë¡œ ë‘ 
	- ë”°ë¼ì„œ ê³µí†µëœ Q-functionìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ê²Œ ë¨
	- ì¦‰ Criticì€ ë‹¤ë¥¸ Actorì˜ ì •ë³´ë¥¼ ì¶”ê°€ë¡œ ë°›ìŒ
	- Actorì˜ ëª¨ë“  ì •ë³´ë¥¼ ì•ˆë‹¤ëŠ” ê²ƒì€ Transition Probì„ ì •í™•íˆ ì•Œ ìˆ˜ ìˆìœ¼ë¯€ë¡œ stationary environmentë¼ê³  ë³¼ ìˆ˜ ìˆìŒ.
	- $$Q_Ï€&i (x, a1, ..., aN)$$ : centralized action-value function
	- $$x = (o1, ..., oN)$$
	- $$A_i$$ê°€ ë°›ëŠ” gradientëŠ” ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ê°œë³„ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸
	- í•™ìŠµ í›„ ActorëŠ” ê°œë³„ì ìœ¼ë¡œ ë™ì‘ê°€ëŠ¥ (ì¶”ë¡ ì—ì„  Q-functionì„ ì“°ì§€ ì•ŠìŒ)
	- ì¦‰ ë‹¤ë¥¸ Actorì˜ policyë¥¼ ì•ˆë‹¤ëŠ” ê°€ì •ì„ ë²„ë¦¬ê²Œ ë¨
	- ë‹¤ì‹œë§í•´ actor-critic policy gradient methodsë¥¼ í™•ì¥í•¨
    - criticì€ ë‹¤ë¥¸ agentsì˜ policyì— ëŒ€í•œ ì •ë³´ë¥¼ í•©ì³ì„œ ì“°ê³ 
    - actorëŠ” local informationì— ëŒ€í•œ ì •ë³´ë§Œ
    - í•™ìŠµì´ ëë‚˜ë©´ execution phaseì—ëŠ” local actorë§Œ ë™ì‘
        - coorperative/competitive ë‘˜ë‹¤ ë™ì‘ ê°€ëŠ¥
		- centralized criticì€ ë‹¤ë¥¸ agentsì˜ decision-making policiesë¥¼ ì‚¬ìš©
    	- ì´ê±¸ í™•ì¸í•´ë³´ì•˜ëŠ”ë° onlineìœ¼ë¡œ ë‹¤ë¥¸ agentì˜ ëŒ€ëµì ì¸ ëª¨ë¸ì„ ë°°ìš°ê³  ìì‹ ì˜ policy í•™ìŠµ ê³¼ì •ì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©

- training agents with an ensemble of policies
	- MA policiesì˜ stabilityë¥¼ í–¥ìƒì‹œí‚¬ ë°©ë²•
	- ë‹¤ì–‘í•œ collaboratorì™€ competitorì˜ policiesì™€ robustí•œ ìƒí˜¸ì‘ìš©ì„ ìš”êµ¬í•¨

- ë‹¤ìŒ ì „ì²´ ì•Œê³ ë¦¬ì¦˜ì„ ì´í•´í•˜ë©´ ë˜ëŠ”ë°
![Image](https://github.com/user-attachments/assets/3f2df994-312f-41ee-8224-6b470dc96acc)
- êµ¬ì„±ìš”ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´
- 		â—‹ Random process
		â—‹ actor net
		â—‹ critic net
		â—‹ target actor net
		â—‹ theta : policyì˜ íŒŒë¼ë¯¸í„°
mu : actor networkë¥¼ ì˜ë¯¸
- ê²½ìŸ ê´€ê³„ì¼ë• ì™ ì§€ Replay bufferë¥¼ ë”°ë¡œ ì¨ì•¼í•˜ì§€ ì•Šë‚˜ ì‹¶ê¸°ë„ í•¨



### [AlphaGo Zero](https://arxiv.org/abs/1712.01815)


| íŠ¹ì§•  | **AlphaGo** | **AlphaZero** |
|--------|------------|---------------|
| **ë°ì´í„° í•™ìŠµ ë°©ì‹** | ì¸ê°„ ê¸°ë³´(ë°ì´í„°) + ìê°€ ëŒ€êµ­(Self-play) | ìˆœìˆ˜ ìê°€ ëŒ€êµ­(Self-play Only) |
| **ì •ì±… í•™ìŠµ** | ì •ì±…ë§(Policy Network) + ì§€ë„í•™ìŠµ(Supervised Learning) | ì •ì±…ë§ê³¼ ê°€ì¹˜ë§ì„ í†µí•©(Policy-Value Network) |
| **ê°€ì¹˜ í•™ìŠµ** | ëª¬í…Œì¹´ë¥¼ë¡œ ë¡¤ì•„ì›ƒ(Monte Carlo Rollout) ê¸°ë°˜ | ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ì§ì ‘ ì˜ˆì¸¡ |
| **íƒìƒ‰ ë°©ì‹** | MCTS + ì •ì±…ë§(Policy) + ê°€ì¹˜ë§(Value) | **MCTS + ê°€ì¹˜ë§(Value) + ì •ì±…ë§(Policy) (ë” íš¨ìœ¨ì )** |
| **ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°** | ì •ì±…ë§(Policy Network) + ê°€ì¹˜ë§(Value Network) **(2ê°œì˜ ë„¤íŠ¸ì›Œí¬)** | **ì •ì±…-ê°€ì¹˜ ë„¤íŠ¸ì›Œí¬(Policy-Value Network, í•˜ë‚˜ë¡œ í†µí•©ë¨)** |
| **ì…ë ¥ íŠ¹ì§•** | ìˆ˜ì‘ì—…ìœ¼ë¡œ ì„¤ê³„ëœ ì—¬ëŸ¬ ë°”ë‘‘ íŠ¹ì§•(feature) ì‚¬ìš© | **ì›ë³¸ ë³´ë“œ ìƒíƒœë§Œ ì‚¬ìš© (End-to-End í•™ìŠµ)** |
| **ì•Œê³ ë¦¬ì¦˜** | DQN(DQN-like) ê¸°ë°˜ ê°•í™”í•™ìŠµ | **ìˆœìˆ˜í•œ ê°•í™”í•™ìŠµ(RL)** (Monte Carlo Tree Search ê¸°ë°˜) |
| **í›ˆë ¨ ë°ì´í„°** | ì¸ê°„ ê¸°ë³´ í•™ìŠµ í›„ RL ì§„í–‰ | **ì˜¤ì§ ê°•í™”í•™ìŠµ(Self-play)ë§Œ ì‚¬ìš©** |

- ë°”ë‘‘ ë¿ ì•„ë‹ˆë¼ ë‹¤ë¥¸ Zero-sum ê²Œì„ì—ë„ ì¼ë°˜í™” (ì°¨ë¡€ë¥¼ ë‘ê³  í•œìª½ì€ ìŠ¹ë¦¬, í•œìª½ì€ íŒ¨ë°°)
- Self-play ë§Œìœ¼ë¡œ í•™ìŠµ
- Monte Carlo Tree Search (MCTS)ëŠ” ê°•í™”í•™ìŠµì€ ì•„ë‹ˆê³  íŠ¸ë¦¬ë¥¼ ìƒ˜í”Œë§í•´ì„œ ìµœì  ê²½ë¡œë¥¼ ì°¾ìŒ



# ëŒ€í‘œ ë¼ì´ë¸ŒëŸ¬ë¦¬

### RLlib
- Rayë¥¼ ë§Œë“ ê³³ì—ì„œ ë§Œë“  RL ë¼ì´ë¸ŒëŸ¬ë¦¬
- ë§ì´ ì“°ëŠ”ì§€ëŠ” ëª¨ë¦„
- í™˜ê²½ ì„¤ì¹˜ëŠ” ë‹¤ìŒì²˜ëŸ¼ í•˜ë©´ ë˜ì—ˆë‹¤
```python
sudo docker run -it --gpus all --net host --shm-size 8G -v $(pwd):/workspace --name ray nvcr.io/nvidia/pytorch:22.12-py3 /bin/bash

pip install -U "ray[rllib]"
pip install gputil
pip install gym==0.23.1 gym-toytext==0.25.0 pygame==2.1.0

```



### AgileRL
- https://github.com/AgileRL/AgileRL
- RLì€ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì„œì¹˜ (HPO, Hyperparameter optimization)ë¥¼ ë§ì´ í•´ë´ì•¼ í•¨
- ê·¸ëŸ°ë° ê·¸ë¦¬ë“œ ì„œì¹˜ê°€ ì•„ë‹ˆë¼ Evolutionary algorithmsì„ ì ìš©í•´ì„œ ë” ë¹¨ë¦¬ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤ê³  í•¨
- ë˜ ë³‘ë ¬í™” ë¶„ì‚° í™˜ê²½ì„ ì§€ì›í•œë‹¤ê³  í•¨!
- ê³„ì† ê´€ë¦¬ëŠ” ë˜ëŠ”ë° 25ë…„1ì›” ê¸°ì¤€ìœ¼ë¡œ ì—„ì²­ í™œë°œí•œê±° ê°™ì§€ ì•ŠìŒ


### WarpDrive
- https://github.com/salesforce/warp-drive
- ë³‘ë ¬ GPU í•™ìŠµ ì§€ì›


# Applications in Other Domains
- LLMì— ê´€ì‹¬ì´ ìˆë‹¤ë©´ RLHFë¥¼ ì•Œ ê²ƒì´ë‹¤. 
- ì´ì²˜ëŸ¼ NLP, Vision, New Drug Discovery, etcì—ì„œ RLê¸°ë²•ì„ ë³€í˜•, í™œìš©í•˜ëŠ” ì¼€ì´ìŠ¤ê°€ ìˆì–´ í¥ë¯¸ë¡­ë‹¤
- ì¢€ ë” ì›í•˜ëŠ” ê²°ê³¼ì— aligní•˜ëŠ” ì—­í• ì— ì œê²©ì´ë‹¤.

## NLP

### InstructGPT (RLHF)â  https://openai.com/index/instruction-following/





### DPO: Your language model is secretly a
reward model

## DeepSeek-R1
- ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒì„ ìœ„í•´ RL ê¸°ë²•ë§Œ ì ìš©
- ë‹¨ìˆœíˆ RLë§Œ ì ìš©í•œ DeepSeek-R1-ZeROëŠ” ì„±ëŠ¥ì´ ì•ˆì¢‹ì•˜ìŒ 
- GRPO(Group Relative Policy Optimization)
	- Group scoreë¥¼ ê¸°ë°˜
- ê·œì¹™ ê¸°ë°˜ Reward model
	- Accuracy Reward
	- Format Reward

- cold-start dataë¥¼ í™œìš©í•œ DeepSeek-R1
	- ë¶ˆì•ˆì •ì„± ì¤„ì„
	- CoTë¡œ êµ¬ì„±ëœ ì†Œê·œëª¨ ë°ì´í„°ì…‹
## Vision
### [DDPO(Denoising Diffusion Policy Optimization)](https://arxiv.org/abs/2305.13301)
- 23ë…„ë„ 5ì›”
- ìƒì„± ì´ë¯¸ì§€ê°€ ìœ ì €ê°€ ì›í•˜ëŠ”ëŒ€ë¡œ ë§ì¶¤ì¸ì§€ íŒë‹¨í•˜ëŠ” ë¦¬ì›Œë“œë¥¼ ë‘ë„ë¡ í•¨
- ë””í“¨ì „ì—ì„œ ìŠ¤í…ë°ŸëŠ” ê³¼ì • ìì²´ëŠ” ì•¡ì…˜ì´ë¼ê³  ë³´ê³  ìµœì¢… ìƒ˜í”Œì´ ë‚˜ì™”ì„ ë•Œ ê°ê° z_tì— ëŒ€í•´ ë¦¬ì›Œë“œë¥¼ ë§¤ê¸°ëŠ” ì‹ìœ¼ë¡œ ë¦¬í”„ë ˆì´ë°
- ì˜ˆë¥¼ ë“¤ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ ë”°ë¥´ê²Œ í•˜ë„ë¡ [LLaVa-BertScoreë¡œ ë¦¬ì›Œë“œ](https://github.com/kvablack/ddpo-pytorch/blob/1958463f020112c9a7bc85768d296daacc2e1b4b/ddpo_pytorch/rewards.py#L118)ë¥¼ êµ¬ì„±í–ˆë”ë‹ˆ í•™ìŠµ ë°ì´í„°ì…‹ì— ì˜ ë‚˜ì˜¤ì§€ ì•Šì„ ë²•í•œ ìƒ˜í”Œë„ ë§Œë“¤ ìˆ˜ ìˆì—ˆë‹¤
- ì½”ë“œëŠ” ì¬ë°Œê²Œë„ LLavaë¥¼ ì„œë²„ì— ëŒë ¤ì„œ callí•˜ê³  ê²°ê³¼ë¥¼ ì–»ëŠ” ë¡œì§ì´ í•™ìŠµ ë£¨í”„ì— ë°˜ì˜ë˜ì–´ ìˆë‹¤. (https://github.com/kvablack/ddpo-pytorch/blob/1958463f020112c9a7bc85768d296daacc2e1b4b/scripts/train.py#L354)

![img1 daumcdn](https://bair.berkeley.edu/static/blog/ddpo/results2.jpg)

### [Diffusion-DPO](https://arxiv.org/abs/2311.12908)
- 23ë…„ë„ 11ì›”
- DPOë¥¼ Diffusionì— ì ìš©í•¨


### [D3PO](https://arxiv.org/abs/2311.13231)
- 23ë…„ë„ 11ì›”


## Generative Model
### [GFlowNet](https://arxiv.org/abs/2111.09266)
- ìƒì„±ëª¨ë¸ê³¼ ê°•í™”í•™ìŠµì˜ êµì°¨ì ì— ìˆìŒ
- ê·¸ë˜í”„ ê¸°ë°˜ì˜ ë°ì´í„° ëª¨ë‹¬ë¦¬í‹°ì— íš¨ê³¼ì ì´ë¼ ì‹ ì•½ ê°œë°œìª½ì—ì„œ ì—°êµ¬ê²°ê³¼ê°€ ë‚˜ì˜¤ëŠ” ê²ƒìœ¼ë¡œ ë³´ì„
- [íŠœí† ë¦¬ì–¼](https://milayb.notion.site/The-GFlowNet-Tutorial-95434ef0e2d94c24aab90e69b30be9b3)ì— ì¹œì ˆí•˜ê²Œ ì„¤ëª…ëœë“¯


# Code Tips

- boundary ê°’ì„ ì•ˆë‹¤ë©´ í•­ìƒ observation spaceë¥¼ normalize
- action spaceë¥¼ normalizeí•´ì„œ

- ë§Œì•½ gymí™˜ê²½ì„ ì‚¬ìš©í•œë‹¤ë©´ ì¸í’‹ì„ rescaleí•´ì•¼í•œë‹¤. 

- ì´ë¯¸ì§€ì˜ ê²½ìš° ì¸í’‹ ê°’ì˜ ë²”ìœ„ê°€ [0, 255]ì´ê³  observationì€ [0, 1]ë¡œ normalizedë˜ì–´ì•¼ í•¨.

https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html


# êµ¬í˜„ íŒ
(1) MDPë¥¼ ì˜ ì •ì˜í•˜ì
- MDP ê° ìš”ì†Œë¥¼ ì˜ ì •ì˜í•˜ì
- ì˜ˆë¥¼ ë“¤ì–´ ë§Œì•½ ì¥ì• ë¬¼ í”¼í•˜ë©´ì„œ ë„ì°© ì§€ì ì— ê°€ëŠ¥ ê²½ìš° Stateë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜
1.Â Agentì—Â ëŒ€í•œÂ ë„ì°©ì§€ì ì˜Â ìƒëŒ€ìœ„ì¹˜Â x,y

2.Â ë„ì°©ì§€ì ì˜Â ë¼ë²¨

3.Â Agentì—Â ëŒ€í•œÂ ì¥ì• ë¬¼ì˜Â ìƒëŒ€ìœ„ì¹˜Â x,y

4.Â ì¥ì• ë¬¼ì˜Â ë¼ë²¨

5.Â ì¥ì• ë¬¼ì˜Â ì†ë„



# Current trends
- Hierarhical learning
	- ë¬¸ì„ ì—¬ëŠ” ì„¸ë¶€ ë™ì‘ì˜ actionì´ ìˆì§€ë§Œ ë¬¸ì„ ì—°ë‹¤ëŠ” ê²ƒì€ í•˜ë‚˜ì˜ ë‹¨ìœ„ë¡œ ë³´ê¸°
	- ë„“ì€ ë‹¨ìœ„ì˜ actionìœ¼ë¡œ ë´„
- Model-based + Model-free learning
- MARL
- Biological RL

# ì¢‹ì€ ì°¸ê³  ìë£Œ
- [A Succinct Summary of Reinforcement Learning](https://arxiv.org/pdf/2301.01379v1.pdf)
- [Pieter Abbeel, CS 287: Advanced Robotics](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa19/)