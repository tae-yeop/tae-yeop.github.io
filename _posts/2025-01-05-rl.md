---
title:  "강화학습 기초정리해보기"
excerpt:  "강화학습 내용 정리"
categories: 
- Reinforcement Learning

tags:
- Reinforcement Learning
 
toc_label: TOC
toc: true
toc_sticky: true
 
date: 2025-01-05

---

# Basics

## RL 구성요소
- 강화학습에서 상정하는 기본적인 셋팅이 있는데 이를 구성하는 요소별로 나열해보면 다음과 같다.

![Untitled](https://github.com/user-attachments/assets/02e07143-6c17-4cad-8052-df3732c57040)


(1) Agent

(2) 환경
- Agent가 상호작용하는 환경
- 환경은 Agent에게 상황(state)를 인풋으로 주고 Agent는 이에 대응되는 행동(action)을 아웃풋으로 낸다
- 환경은 다시 이런 action을 받아서 Agent에게 새로운 상황(state)을 리턴시킴 (이를 transition이라고 함)
- GAN에 대해 익숙하다면 사실상 GAN과 비슷한 셋업이다. 다만 GAN에선 환경 역할을 하는 Discriminator가 동시에 학습된다는 점
- 이때 state의 모든 정보를 줄지 부분만 줄지에 따라 : fully vs partially observable

(3) policy 
- mapping from states to actions. : bid and ask policy
- policy $\pi_{\theta}(a | s)$



(4) reward signal
- deviation from

(5) value functions
- action value function for policy $\pi$
- state value function for policy $\pi$
(6) MDP
- 위에 말한 전체 요소를 좀 더 수학적인 언어로 명확하게 정리한 개념
- finite Markov Decision Process (MDP) 기준으로 다음과 같다
    - a finite set of states $S$
    - b. finite set of actions $A$
    - c. a discount rate $\gamma \in [0,1] $
    - d. a finite set of rewards $R$
    - e. the one-step dynamics of the environemnt $\forall s,r',a,r :  p(s',r | s,a) = \Bbb{P} (S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$
    - Agnet knows : a,b,c
    - Agent does not know : d,e
- Agent는 a를 취하면서 r를 최대화하려고 한다

- Transition Prob $P(s'|s,a)$이 고정 : Stationary Environment

## Dynamic Programming
- 큰 문제를 작게 나눠서 풀고 조금씩 확장해서 더 큰 문제를 푼다
- constraints의 최소 단위를 어떻게 정의하느냐가 문제가 될 수 있음
- 시퀀스 매칭도 DP로 푼다
- 최적의 문제를 뭔가 계산하고 싶은데 문제가 너무 크면 잘게 쪼개서 해결 (Divide and conquer)


## DP의 한계점
1. Computation complexity

2. Curse of dimension
- 작은 격자의 미로 찾기만 해도 state, action 경우의 수가 크다
- 이를 Q-table로 하는 건 무리 -> Q-function을 근사화
3. Full description of environment
- 현실에선 MDP라는걸 완벽하게 알 수 없고 인터액션으로 알아내야 함
- 이 때문에 

## Exploration-Exploitation Dilema
- 단기적으로 보상을 얻는 전략을 만드는 것은 상대적으로 쉽지만 장기적인 전략을 만드는 건 힘들 수 있다


# 기초 알고리즘

전통적인 Optimal planning으로는 힘들다
정확하긴 하나 학습 효율성이 낮다 -> $p(s_{t+1} | s_t, a_t)$를 알아야 하고 차원이 커지면 힘듬 (모든 state에 대해 iteration을 돌려야 해서)
$$
V_{k+1}(s)= \max_a∑_{s'}P(s'∣s,a)[R(s,a)+γV_k(s')]
$$
- V를 다 찾고나서 Policy를 얻음
$$
V_{k+1}(s)= \max_a∑_{s'}P(s′∣s,a)[R(s,a)+γV(s′)]
$$






대신에 샘플링 기반의 강화학습 방법이 필요함




# 분류

- 항상 분류는 기준점을 어떻게 두느냐에 따라 다르게 분류할 수 있다.
- 먼저 업데이트 time-step에 따라 3 가지 큰 분류를 할 수 있다
	- 위에서 살펴본 DP 방식, MC 방식, TD 방식
	- 알고리즘 시간에 배우는 DFS, BFS의 관계와 비슷한 철학
	- leaf까지 쭉 먼저 내려갈 것인니 아니면 주변부를 탐색하고 결정한것인지
- 환경에 대해 알고 있는지에 따른 분류
	- Model-free vs Model-based
- 업데이트하고자 하는 대상에 따른 분류
	- Value-based vs Policy-based

![img1 daumcdn](https://spinningup.openai.com/en/latest/_images/rl_algorithms_9_15.svg)


- 먼저 큰 분류로는 Model-free RL vs Model-based RL
	- agent가 환경이라는 모델의 정보를 접근할 수 있는지 여부로 구분
	- 환경 : $s_{t+1}, r = env(s_t, a_t)$
	- 여기서 모델이란 $p(s_{t+1} | s_t, a_t)$ 이라고 보면 됨
- Model-Free는 내부적인 동작 ( inner working)보다는 리워드를 추정하기 위해 샘플링을 신경씀

$$
\pi_{\theta}(a_t | s_t) \\
\xcancel{p(s_{t+1} | s_t, a_t)}
$$
$$
\pi_{\theta}(a_t | s_t) \to \tau = \{ s_1, a_1, s_2, a_2, ..., s_T, a_T\} \\
\max_{\pi_{\theta}}\Bbb{E}_{\tau \sim \pi_{\theta}}[\sum_t \gamma^t r(s_t, a_t)] \\
$$

- 반면 Model-based는 모델에 집중함
$$
\xcancel{\pi_{\theta}(a_t | s_t)} \\
p(s_{t+1} | s_t, a_t) : \text{model}
$$
$$
\tau = \{ s_1, a_1, s_2, a_2, ..., s_T, a_T\} \\
\min_{a_1, ..., a_T} \sum_{t-1}^T c(s_t, a_t) : \text{cost} \\
 \text{such that} \quad s_t = f(s_{t-1}, a_{t-1}) : \text{model}
 $$

- 환경에 대한 정보를 통해 planning을 할 수 있어서 sample efficiency가 높다
- agent는 planning 결과를 학습한 policy에 반영할 수 있음
- 알파제로가 대표적인 Model-based임
- 문제는 bias가 생기면 학습한 환경에서만 잘 동작하고 real env에선 sub-optimal하게 동작
- Model-based응 sampling efficieny가 낮지만 학습이 더 쉬움

- Model Free는 다시 Policy기반 접근법과 Value 기반(Q-Learning) 접근법으로 크게 나눌 수 있다
- Value-based

![Image](https://github.com/user-attachments/assets/b89f4d0a-7389-4e31-9cad-ea9fe12c640d)
- Value-based에서도 어찌 되었든 Optimal Policy를 찾게됨

- Policy-based
![Image](https://github.com/user-attachments/assets/8e4edb79-14a3-4adf-81e7-c5a6356235cb)
	- optimal value function을 찾기 않고 바로 optimal policy를 찾는다
	- 즉 action을 바로 아웃풋으로 내놓는 네트워크를 학습하자

![Image](https://github.com/user-attachments/assets/4bd932dd-4773-4363-bcfe-cb3a8862e707)
- action도 아웃풋을 어떻게 설정할 것인지 고민할 수 있다. 
- softmax를 써서 각 action class에 대한 확률 값을 내는 식으로 할 수 있다 : Stochastic policy
- argmax를 써서 가장 확률이 높은 것만 취하도록 할수도 있다 : Determinstic policy
- 만약 categorical하지 않고 continuous하다면 => tanh사용함

![Image](https://github.com/user-attachments/assets/920aa96a-cde8-4f49-a2c4-30a6f1848b5f)
- 기대값을 최대화하는게 목표인데 네트워크 weight $\theta$ 찾는게 포인트
- $\theta$가 결국 action을 만들고 이는 다시 reward를 얻는 것이므로 기대값을 $\theta$의 함수로 표현가능
## MC 방식
- An episode가 collect되어야 Update(Episodic task 사용)
- Return에 기반하여 업데이트

- 초기에는 Equal probable random policy를 가짐 -> 랜덤 행동
- 환경은 이러한 랜덤 행동을 하는 Agent에 next_state와 reward라는 정보를 주게 됨
- 업데이트를 거쳐서 $\pi_{\theta}^{*}$를 얻게 됨

### MC Prediction

### MC Control
![Image](https://github.com/user-attachments/assets/5650b5a2-52ba-4d71-8297-b20b18defb0d)
- Policy를 최적화함
- 두 가지 과정 : Policy Evaluation + Policy Improvment
	- 현재의 policy $\pi$를 이용해서 episode $(S_0, A_0, R_1, \dots, S_T)$를 모음
	- $Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha(G_t - Q(S_t, A_t))$
	- $Q(S_t, A_t)$에는 expected return에 대해 agent가 추정한 정보가 반영되어 있음
	- 만약 return $G_t \neq Q(S_t, A_t)$ 라면 업데이트하게 됨
- 구체적으론 3가지 방법
	- Incremental mean
	- Constant-alpha
	- Constant $\alpha$ MC Control



## TD
- 하나의 스텝을 보고 업데이트 수행
- 업데이트 식은 다음과 같음
$$
V(s_t)←V(s_t)+α[r_t+γV(s_{t+1})−V(s_t)]
$$


## N-step bootstrapping
![Image](https://github.com/user-attachments/assets/6b4f2b18-1f87-438b-9e44-2b817af0c60d)

- TD와 MC의 중간
- TD를 사실 1-step bootstrapping이라고 볼 수 있음
- 좀 더 환경과 상호작용하여 경험을 쌓기 때문에 Bias를 줄일 수 있다



## TD Control : SARSA

$ Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha(R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) )$

- $R_{t+1} + \gamma Q(S_{t+1}, A_{t+1})$ : Interprete as target (TD target, Bellman eq RHS)
- $Q(S_t, A_t)$ : Interprete as prediction (Bellman eq LHS)

- 인공신경망의 경우 SGD를 돌려야 하므로 MSE로 생각
$ \text{MSE} = (y-\hat{y})^2 = ( R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t))^2$


SARSA를 구현하는 방식이 2가지이다.

(1) on-policy
- 실제할 행동에 대한 aciton-value 추정치를 $Q(s', a')$에 사용
- 가장 최근 버전의 policy를 따라서 행동하면서 수행한 데이터만으로 업데이트하는 것
- epsilon-greedy로 하게된다면 Explorative action(a) + Explorative estimation(a')
- 타겟이 되는것 까지 같이 배움 -> 불안정 -> sample간의 correlation이 높다고 표현

(2) off-policy (Q-learning)
- 학습 도중 아무 시점에 수집한 데이터를 이용해서 q-function을 업데이트
- s'에서 가장 최대값을 주는 것을 택함
- Explorative action(a) + Confident estimation(a')
- R이 잘 주어질 때는 추정치가 잘 맞으므로 이게 더 효율적
- 초반 Over-confident -> Risky
- 알파고1이 이 방식으로 했는데 티처를 잘 셋팅해두었기 때문에 이 방식이 문제가 되지 않음

- 이를 Neural Net으로 구현한게 DQN이다. 
	- 추가적인 트릭으로 Replay Buffer를 사용함
	- 이는 레어한 시퀀스가 유용할 수 있지만 덜 업데이트 되는것을 방지할 수 있음
	- Action $A_{t}$와 다음 State $S_{t+1}$는 correlated되어 있다고 표현함 
	- 이후 Replay Buffer에서 랜덤하게 뽑아냄
	- 만약 다 찼으면 옛날거 부터 삭제
	- 이전 Policy에서 만듬 샘플을 이용 -> 현재 학습하는 Policy와 다름

From Q-function upate :
$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma \max_{a'}{Q(S_{t+1}, a')} - Q(S_t, A_t))
$$

We can derive loss function for DQN:
$$
MSE = (R_{t+1} + \gamma \max_{a'}{Q(s', a', \theta)} - Q(s', a', \theta))^2
$$
Problem with bootstrap : target for update keeps chaning. To prevent this, we need fixed network for some duration.
$$
\therefore MSE = (R_{t+1} + \gamma \max_{a'}{Q(s', a', \theta^{-})} - Q(s', a', \theta))^2
$$

- 이를 통해 optimal action-value function $Q^{*}(s, a)$를 근사화

## DQN ImpImprovement
### Double DQN
- Target을 내는 네트워크 자체도 학습하면 불안정함 (배가 산으로...)
- 아예 네트워크를 분리해서 cross-reference함.


- Fixed Q-target
- priotized experience replay
- dueling networks

## C51
- distributed RL
- reward가 하나가 아님 -> 분포로 생각 (불확실성 반영)


## Value-based
- approximate value function : $V_pi(s), Q_pi(s,a), A_pi(s,a)$


### Deep-SARSA
- Q-function을 NN으로 근사
- 학습을 물론 SGD를 사용

### DQN

### Exploration-Exploitation Trade-off
- Greedy Policy

- Epsilon Greedy Policy
$$
\pi(a|s) \begin{cases} 1-\epsilon + \frac{\epsilon}{|\mathcal{A}(s)|} & \text{if} \quad a \quad \text{maximizes} \quad Q(s,a) \\ \frac{\epsilon}{|\mathcal{A}(s)|} & \text{else} \end{cases}
$$



## Policy-based
- (s,a)에서 r을 주는 상황이 관심이 없고 그냥 a만 중요한 경우
- $\pi_{\theta}(s,a)$ 직접 근사화하자

```python
def bulid_model(self):
  model = Sequential()
  model.add(Dense(24, input_dim=self.state_size, activation='relu'))
  model.add(Dense(24, activation='relu'))
  model.add(Dense(self.action_size, activation='softmax'))
  return model
```


- 신경망 학습 논리만 충실히 따르자
- $\Bbb{E}[r]$ 를 최대화하도록 $\pi_{\theta}(s,a)$를 학습
- 확률값이 나오기 때문에 Greedy Policy, Epsilon Greedy Policy 같은게 내재되어 있음

- 그리고 확률적으로 action을 선택해서 local minima에 빠지지 않게
- future reward에 대한 식을 objective로 잡음
미분해서 보니깐 Q 네트워크가 다시 필요하더라

- Objective function J를 다음과 같이 모델링하고 gradient를 찾자.

$$
J(\theta) = \Bbb{E}_{\tau \sim \pi_{\theta}}[R(\tau)] \\
\nabla_{\theta}J(\theta) = \Bbb{E}_{\tau \sim \pi_{\theta}}[\sum_{t=0}^T \nabla_{\theta} \log{\pi_{\theta} (a_t | s_t)} R(\tau)] \\
\nabla_{\theta}J(\theta) = \Bbb{E}_{\tau \sim \pi_{\theta}}[\sum_{t=0}^T \nabla_{\theta} \log{\pi_{\theta} (a_t | s_t)} \sum_{t'=t}^T R(s_{t'},a_{t'},s_{t'+1} )]
$$
- 제일 안쪽 sum에 있는 reward의 합을 reward-to-go policy gradien
$$
\hat{R_t} = \sum_{t'=t}^T R(s_{t'},a_{t'},s_{t'+1} )
$$


f_w가 valu function인데 

Reward $G$와 $J$의 차이
	- 
G는 single episode에서 모이는 return이고 J는 expected return이다. 
실제 theta가 똑같더라도 환경에서 오는 랜덤성과 policy가 stochastic하다면 
에피소드가 끝나고 얻어지는 return G는 일정하지 않을 수 있다.
따라서 expected return J의 정확한 추정값은 아님 실전에선 충분히 좋다

- Hill climbing, Simulated Annealing(Random restarts) 등 기존의 알고리즘 또한 Policy 방식으로
생각할 수 있음
- 기존의 Black-box optimization 방법론들도 Policy의 $\theta$를 찾는 식으로 적용 가능 (Hill-climbing like, Cross-Entropy method, Evolution Strategy)


- 대신 딥러닝 학습을 이용할 것이므로 Policy gradient method에 주목함


### Stochastic PG vs Deterministic PG
- $\pi_{\theta}(s, a)$ vs $a=\mu_{\theta}(s)$
- 운전상황 : 이런거저런거 보단 compact하게 action을 취해야함
- 신경망의 아웃풋이 continuos 값을 냄 => continuous action space 구현 가능


Policy gradient는 Supervised learning과 비슷함



### Deterministic PG
- David Silver(알파고 제작 총괄자)
- Q function : 현재 상황이 얼마자 좋은지를 판단함 => critic이라고 부름
- Actor : a를 내놓음


### Vanila Policy Gradient



### REINFORCE
![Image](https://github.com/user-attachments/assets/2c22c1bb-4cd1-45fd-a180-1ae0e1cfda5c)
- Trajectory $\tau = \{s_0, a_0, s_1, a_1, \dots, s_H, a_H, s_{H+1} \}$
- $R(\tau) = r_1 + r_2 + \dots + r_H + r_{H+1}$ : 특정 trajectory의 리턴
- expected return $U(\theta)$를 최대화하는 방향으로 trajectory를 $\theta$로 부터 샘플링하여 gradient를 구해서 최적화하자
- 해당 trajectory가 나올 확률 $P(\tau ; \theta)$는 policy에서 나오는 확률을 곱으로 표현하고 이를 다시 log를 취하면 합으로 표현됨 : $∑_{t=0}^T R_t∇_θlogπ_θ(a_t∣s_t)$


- 단점
	- (1) update가 비효율적 : policy에서 trajectory 뽑고 다시 안씀
	- (2) noisy gradient : 확률적으로 action을 취하기 때문에 샘플링한 trajectory가 실제 policy $\pi$를 반영하지 못할 수도
	- (3) unclear credit assignment : total return만 보기때문에 bad action이 포함될수 있음
	- (4) Return을 사용하기 때문에 에피소드가 끝나야 학습가능 


Monte-Carlo sampling

sampling complexity efficiency

noisy varaicne gradient

‘godd’ and ‘bac’ action → variance

culumative returns +  variablity of trajectories → gradient

Maximize Expected Return

### [TRPO 15.02](https://arxiv.org/abs/1502.05477)
- PG에서 안정성을 높인 알고리즘
- step size가 너무 크면 연쇄적인 문제점이 생김 : Bad policy → Bad action → No signal → Thus terrible policy
- line search를 통해 최적의 step size를 찾음 (비용은 증가됨됨)
- trust regiond이라는 개념을 통해 기존 Policy에서 너무 벗어나지 않는 regularization을 도입
$$L^{TRPO}(θ)=E[ π θ old(a∣s)π θ(a∣s)A π θ old(s,a)] 
$$
- TRPO is powerful but computationally expensive due to second-order optimization.
- Trust Region을 유지하려고 Second-Order Optimization(Natural Gradient)을 사용
- 하지만 이런 trust region constraint을 부여하는게 쉽지 않음음



PPO removes the need for second-order optimization by using a clipped surrogate loss.
PPO is more scalable and can be used with first-order optimizers like Adam.
Clipping in PPO replaces the KL divergence constraint in TRPO, making it easier to implement and more efficient.
PPO_v2 : Clipped Surrogate Loss


### [PPO](https://arxiv.org/abs/1707.0634)
- REINFORCE의 단점을 개선
- TRPO의 단점을 개선
- performance를 최대화하면서 간접적으로 policy를 업데이트한다
    - surrogate objective function
    - estimate of $J(\pi_{\theta})$

- 하나의 trajectory에 오는 노이즈를 낮추기 위해 더 많은 trajectory를 샘플링함 (병렬처리하면 더 효율적)

1. First, collect some trajectories based on some policy $\pi_{\theta}$, and initialize theta prime $\theta' = \theta$

2. Next, compute the gradient of the clipped surrogate function using the trajectories

3. Update $\theta'$using gradient ascent $\theta'\leftarrow \theta' + \alpha \nabla_{\theta'} L_{sur}^{clip} (\theta', \theta)$

4. Then we repeat step 2-3 without generating new trajectories. Typically, step 2-3 are only repeated a few times

5. Set $\theta = \theta'$, go back to step 1, repeat.



### TD3


## Actor-Critic
![Image](https://github.com/user-attachments/assets/c148b185-dbf7-40d0-bbde-74f972a68f04)

- Value-based + Policy-based Hybrid
- Value-based (DQN)은 보통 3가지를 근사할고 함 (state-value function, action-value function, advantage function)
- Policy-based에선 Stochastic policy의 경우 확률은 Determinstic policy의 경우 action을 내놓음.

![Image](https://github.com/user-attachments/assets/d4bb3ce0-d739-4c2d-a035-2f0eaf828059)

![Image](https://github.com/user-attachments/assets/04b288eb-3640-4daf-813a-23f5a41e6c70)

- Critic이 baseline을 제공하면서 boostrapping을 할수 있게 해준다 -> Policy-based의 variance를 줄이자.
- 기계학습에서 항상 나오듯이 bias-variance trade-off가 있는데 특히 강화학습에선 Return을 계산하는데 발생

- Actor = Policy netowrk 근사화
	- action_prob를 내놓음
- Critic = Q-function 근사화화
	- Actor의 행동이 좋은지를 판단

### A3C (Asynchronous Advantage Actor-Critic)
- gradient ascent를 수행하여 직접 performance를 최대화
- 하나의 네트워크 weight를 Actor와 Critic이 공유한다.



### A2C





### DDPG
- Policy gradient theorem에 의해 Q를 구해서 $\pi$ 업데이트 수식에 대입
- off-policy actor-critic
- target network
- ActionNet : $S \to A$
```python
class Actor(nn.Module)
	__init__(cfg)
		super().__init__()
		# [state_size] -> [hidden_size] -> [action_size]
		self.model = ...
	forward(state)
		x = self.model(state)
		return F.tanh(x)

class Critic(nn.Module)
	__init__(cfg)
		# [state_size] -> [hidden_size] + [action_size] -> [hidden_size] -> [1]
	forward(state, action)
		x = self.model1(state)
		x = torch.cat((x, action), dim=-1)
		self.model2(x)
		
	
```
```python
for i in range(num_episodeds):
	env.reset()
	agent.reset()

	state = env.initial_state()
	score = np.zeros(num_agents)
	for t in range(timestep):
		action = agent(state)
		next_state, rewards, done = env.step(action)
		agent.step(state, action, reward, next_state, done)
		state = next_state
		score += reward

		if np.any(dones):
			break

	scores.append(np.mean(score))
	if i % CKT_FREQ == 0 or i == num_episodes:
		save()
	
```

### [D4PG, 18.04](https://arxiv.org/abs/1804.08617)
- 기대값보단 리워드의 분포를 모델링
- 분산 학습 접근법 : Critic 학습을 더 향상 + Replay buffer를 Actor끼리 공유



### SAC
- stochastic policy optimization + DDPG-style approaches(clipped double Q function + target policy smoothing)
- off-policy
- entropy regularization term
	- $H(P(x)) = E_{x \sim p} [-logP(x)]$
	- maximize the entropy of policy
	- expceted return(exploitation) vs entropy (randomness in the policy, exploration)
- Policy network $\pi_{\theta}$, Q-function : $\phi_1, \phi_2$, replay buffer : $D$
- target Q-function : $\phi_{1, t} \leftarrow \phi_1, \phi_{2,t} \leftarrow \phi_2$



# Multi-agent(MARL)
- 자율주행 자동차(SDC)의 상황을 생각하면 다른 자동차를 Agent라고 볼 수 있음
- 가장 현실세계에 가까운 셋팅이 바로 MA 상황
- 서로 협동(Cooperative)을 하거나 경쟁(Competitive), 복합적인 상황(Mixed Environments)일수도
- 장점을 뭘까?
	- 만약 다른 자동차들의 experience로 모아서 학습을 할 수 있다면 더 풀스펙스트럼을 학습할 듯
- Markov Game Framework
$ <n, S, A_1, \dots, A_n, O_1, \dots, O_n, R_1, \dots, R_n, \pi_1, \dots, \pi_n, T> $
- $n$ : # of agents
- $S$ : set of 
- $A$ : $A_1 \times A_2 \dots \times A_n $
- $R_i$ : $ S \times A \to R$ (각 agent i에 대한 reward)
- $\pi_i$ : $O_i \to A_i$ $ (각 agent i에 대한 policy)
- $T$ : $S \times A \to S$ (State transition function)
- State transition 또한 Markovian 가정 (Next state는 오직 present state와 여기서 취하는 Joint action에 의해서만 결정)

## 학습
- Single-agent RL algorithm vs Meta Agent Approach
- Either leads to non-stationarity or Large joint action space

### [MADDPG](https://arxiv.org/abs/1706.02275)
- DDPG를 MA기반으로 확장
- centralized training with decentralized execution
	- 메인 Critic을 하나 두고 Actor를 개별적으로 둠
	- 따라서 공통된 Q-function으로 업데이트하게 됨
	- 즉 Critic은 다른 Actor의 정보를 추가로 받음
	- Actor의 모든 정보를 안다는 것은 Transition Prob을 정확히 알 수 있으므로 stationary environment라고 볼 수 있음.
	- $$Q_π&i (x, a1, ..., aN)$$ : centralized action-value function
	- $$x = (o1, ..., oN)$$
	- $$A_i$$가 받는 gradient는 다르기 때문에 개별적으로 업데이트
	- 학습 후 Actor는 개별적으로 동작가능 (추론에선 Q-function을 쓰지 않음)
	- 즉 다른 Actor의 policy를 안다는 가정을 버리게 됨

- 다음 전체 알고리즘을 이해하면 되는데
![Image](https://github.com/user-attachments/assets/3f2df994-312f-41ee-8224-6b470dc96acc)
- 구성요서는 다음과 같이
- 		○ Random process
		○ actor net
		○ critic net
		○ target actor net
		○ theta : policy의 파라미터
mu : actor network를 의미
- 경쟁 관계일땐 왠지 Replay buffer를 따로 써야하지 않나 싶기도 함



### [AlphaGo Zero](https://arxiv.org/abs/1712.01815)


| 특징  | **AlphaGo** | **AlphaZero** |
|--------|------------|---------------|
| **데이터 학습 방식** | 인간 기보(데이터) + 자가 대국(Self-play) | 순수 자가 대국(Self-play Only) |
| **정책 학습** | 정책망(Policy Network) + 지도학습(Supervised Learning) | 정책망과 가치망을 통합(Policy-Value Network) |
| **가치 학습** | 몬테카를로 롤아웃(Monte Carlo Rollout) 기반 | 신경망을 사용하여 직접 예측 |
| **탐색 방식** | MCTS + 정책망(Policy) + 가치망(Value) | **MCTS + 가치망(Value) + 정책망(Policy) (더 효율적)** |
| **네트워크 구조** | 정책망(Policy Network) + 가치망(Value Network) **(2개의 네트워크)** | **정책-가치 네트워크(Policy-Value Network, 하나로 통합됨)** |
| **입력 특징** | 수작업으로 설계된 여러 바둑 특징(feature) 사용 | **원본 보드 상태만 사용 (End-to-End 학습)** |
| **알고리즘** | DQN(DQN-like) 기반 강화학습 | **순수한 강화학습(RL)** (Monte Carlo Tree Search 기반) |
| **훈련 데이터** | 인간 기보 학습 후 RL 진행 | **오직 강화학습(Self-play)만 사용** |

- 바둑 뿐 아니라 다른 Zero-sum 게임에도 일반화 (차례를 두고 한쪽은 승리, 한쪽은 패배)
- Self-play 만으로 학습
- Monte Carlo Tree Search (MCTS)는 강화학습은 아니고 트리를 샘플링해서 최적 경로를 찾음



# 대표 라이브러리

### RLlib
- Ray를 만든곳에서 만든 RL 라이브러리
- 많이 쓰는지는 모름
- 환경 설치는 다음처럼 하면 되었다
```python
sudo docker run -it --gpus all --net host --shm-size 8G -v $(pwd):/workspace --name ray nvcr.io/nvidia/pytorch:22.12-py3 /bin/bash

pip install -U "ray[rllib]"
pip install gputil
pip install gym==0.23.1 gym-toytext==0.25.0 pygame==2.1.0

```



### AgileRL
- https://github.com/AgileRL/AgileRL
- RL은 하이퍼 파라미터 서치 (HPO, Hyperparameter optimization)를 많이 해봐야 함
- 그런데 그리드 서치가 아니라 Evolutionary algorithms을 적용해서 더 빨리 수행할 수 있다고 함
- 또 병렬화 분산 환경을 지원한다고 함!
- 계속 관리는 되는데 25년1월 기준으로 엄청 활발한거 같지 않음


### WarpDrive
- https://github.com/salesforce/warp-drive
- 병렬 GPU 학습 지원


# Applications in Other Domains
- LLM에 관심이 있다면 RLHF를 알 것이다. 
- 이처럼 NLP, Vision, New Drug Discovery, etc에서 RL기법을 변형, 활용하는 케이스가 있어 흥미롭다
- 좀 더 원하는 결과에 align하는 역할에 제격이다.

## NLP

### InstructGPT (RLHF)⁠ https://openai.com/index/instruction-following/





### DPO: Your language model is secretly a
reward model

## DeepSeek-R1
- 추론 능력 향상을 위해 RL 기법만 적용
- 단순히 RL만 적용한 DeepSeek-R1-ZeRO는 성능이 안좋았음 
- GRPO(Group Relative Policy Optimization)
	- Group score를 기반
- 규칙 기반 Reward model
	- Accuracy Reward
	- Format Reward

- cold-start data를 활용한 DeepSeek-R1
	- 불안정성 줄임
	- CoT로 구성된 소규모 데이터셋
## Vision
### [DDPO(Denoising Diffusion Policy Optimization)](https://arxiv.org/abs/2305.13301)
- 23년도 5월
- 생성 이미지가 유저가 원하는대로 맞춤인지 판단하는 리워드를 두도록 함
- 디퓨전에서 스텝밟는 과정 자체는 액션이라고 보고 최종 샘플이 나왔을 때 각각 z_t에 대해 리워드를 매기는 식으로 리프레이밍
- 예를 들어 프롬프트를 잘 따르게 하도록 [LLaVa-BertScore로 리워드](https://github.com/kvablack/ddpo-pytorch/blob/1958463f020112c9a7bc85768d296daacc2e1b4b/ddpo_pytorch/rewards.py#L118)를 구성했더니 학습 데이터셋에 잘 나오지 않을 법한 샘플도 만들 수 있었다
- 코드는 재밌게도 LLava를 서버에 돌려서 call하고 결과를 얻는 로직이 학습 루프에 반영되어 있다. (https://github.com/kvablack/ddpo-pytorch/blob/1958463f020112c9a7bc85768d296daacc2e1b4b/scripts/train.py#L354)

![img1 daumcdn](https://bair.berkeley.edu/static/blog/ddpo/results2.jpg)

### [Diffusion-DPO](https://arxiv.org/abs/2311.12908)
- 23년도 11월
- DPO를 Diffusion에 적용함


### [D3PO](https://arxiv.org/abs/2311.13231)
- 23년도 11월


## Generative Model
### [GFlowNet](https://arxiv.org/abs/2111.09266)
- 생성모델과 강화학습의 교차점에 있음
- 그래프 기반의 데이터 모달리티에 효과적이라 신약 개발쪽에서 연구결과가 나오는 것으로 보임
- [튜토리얼](https://milayb.notion.site/The-GFlowNet-Tutorial-95434ef0e2d94c24aab90e69b30be9b3)에 친절하게 설명된듯


# Code Tips

- boundary 값을 안다면 항상 observation space를 normalize
- action space를 normalize해서

- 만약 gym환경을 사용한다면 인풋을 rescale해야한다. 

- 이미지의 경우 인풋 값의 범위가 [0, 255]이고 observation은 [0, 1]로 normalized되어야 함.

https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html


# 구현 팁
(1) MDP를 잘 정의하자
- MDP 각 요소를 잘 정의하자
- 예를 들어 만약 장애물 피하면서 도착 지점에 가능 경우 State를 다음과 같이 정의
1. Agent에 대한 도착지점의 상대위치 x,y

2. 도착지점의 라벨

3. Agent에 대한 장애물의 상대위치 x,y

4. 장애물의 라벨

5. 장애물의 속도



# Current trends
- Hierarhical learning
	- 문을 여는 세부 동작의 action이 있지만 문을 연다는 것은 하나의 단위로 보기
	- 넓은 단위의 action으로 봄
- Model-based + Model-free learning
- MARL
- Biological RL

# 좋은 참고 자료
- - A Succinct Summary of Reinforcement Learning
    - https://arxiv.org/pdf/2301.01379v1.pdf